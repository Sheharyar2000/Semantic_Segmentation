{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uB9tZW_oeiHe",
        "outputId": "eb71aee1-9aff-4b20-8054-3f41ec6ad381"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Paths to your zip files in Drive\n",
        "gta5_zip = \"/content/drive/MyDrive/Semantic_Segmentation/GTA5.zip\"\n",
        "cityscapes_zip = \"/content/drive/MyDrive/Semantic_Segmentation/Cityscapes.zip\"\n",
        "\n",
        "# Destination folders\n",
        "gta5_extract_path = \"/content/datasets/GTA5\"\n",
        "cityscapes_extract_path = \"/content/datasets/Cityscapes\"\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(gta5_extract_path, exist_ok=True)\n",
        "os.makedirs(cityscapes_extract_path, exist_ok=True)\n",
        "\n",
        "# Extract GTA5\n",
        "with zipfile.ZipFile(gta5_zip, 'r') as zip_ref:\n",
        "    zip_ref.extractall(gta5_extract_path)\n",
        "\n",
        "# Extract Cityscapes\n",
        "with zipfile.ZipFile(cityscapes_zip, 'r') as zip_ref:\n",
        "    zip_ref.extractall(cityscapes_extract_path)\n",
        "\n",
        "print(\"✅ GTA5 dataset extracted\")\n",
        "print(\"✅ Cityscapes dataset extracted\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1laEOPYgLko",
        "outputId": "b0e05f5a-c199-4b2e-f835-408fd68ca656"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ GTA5 dataset extracted\n",
            "✅ Cityscapes dataset extracted\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Fix Cityscapes structure\n",
        "base_city = \"/content/datasets/Cityscapes\"\n",
        "wrong_city_nested = os.path.join(base_city, \"Cityscapes\", \"Cityspaces\")\n",
        "\n",
        "if os.path.exists(wrong_city_nested):\n",
        "    for sub in [\"gtFine\", \"images\"]:\n",
        "        src = os.path.join(wrong_city_nested, sub)\n",
        "        dst = os.path.join(base_city, sub if sub == \"gtFine\" else \"leftImg8bit\")\n",
        "        shutil.move(src, dst)\n",
        "    shutil.rmtree(os.path.join(base_city, \"Cityscapes\"))\n",
        "    print(\"✅ Fixed Cityscapes structure\")\n",
        "\n",
        "# Fix GTA5 structure\n",
        "base_gta = \"/content/datasets/GTA5\"\n",
        "wrong_gta_nested = os.path.join(base_gta, \"GTA5\")\n",
        "\n",
        "if os.path.exists(wrong_gta_nested):\n",
        "    for sub in [\"images\", \"labels\"]:\n",
        "        shutil.move(os.path.join(wrong_gta_nested, sub), os.path.join(base_gta, sub))\n",
        "    shutil.rmtree(wrong_gta_nested)\n",
        "    print(\"✅ Fixed GTA5 structure\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7c3iV8MiwiZ",
        "outputId": "f042be28-8b1e-445b-fc0e-201434439b18"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Fixed Cityscapes structure\n",
            "✅ Fixed GTA5 structure\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining Dataset Classes with Augmentations"
      ],
      "metadata": {
        "id": "n-Gmzpknjwpu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# ✅ GTA5 → Cityscapes label remapping\n",
        "GTA5_TO_CITYSCAPES = {\n",
        "    7: 0, 8: 1, 11: 2, 12: 3, 13: 4, 15: 5, 17: 6, 19: 7, 20: 8,\n",
        "    21: 9, 22: 10, 23: 11, 24: 12, 26: 13, 27: 14, 28: 15,\n",
        "    31: 16, 32: 17, 33: 18\n",
        "}\n",
        "\n",
        "class GTA5Dataset(Dataset):\n",
        "    def __init__(self, root, transform=None, target_transform=None, augment=False):\n",
        "        self.image_dir = os.path.join(root, \"images\")\n",
        "        self.label_dir = os.path.join(root, \"labels\")\n",
        "        self.images = sorted(os.listdir(self.image_dir))\n",
        "        self.labels = sorted(os.listdir(self.label_dir))\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        self.augment = augment\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def encode_labels(self, mask):\n",
        "        remapped = np.full_like(mask, 255)\n",
        "        for gta_id, city_id in GTA5_TO_CITYSCAPES.items():\n",
        "            remapped[mask == gta_id] = city_id\n",
        "        return remapped\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.image_dir, self.images[idx])\n",
        "        mask_path = os.path.join(self.label_dir, self.labels[idx])\n",
        "\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        mask = Image.open(mask_path)\n",
        "\n",
        "        if self.augment and random.random() < 0.5:\n",
        "            img = transforms.functional.hflip(img)\n",
        "            mask = transforms.functional.hflip(mask)\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        if self.target_transform:\n",
        "            mask = self.target_transform(mask)\n",
        "            mask = self.encode_labels(mask.squeeze().numpy())\n",
        "            mask = torch.from_numpy(mask).long().unsqueeze(0)\n",
        "\n",
        "        return img, mask\n"
      ],
      "metadata": {
        "id": "qkvuvqkDjwav"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting up the Cityscapes Validation Dataset and Dataloaders"
      ],
      "metadata": {
        "id": "xlApwq19ldXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# ✅ Transformations for images and masks\n",
        "image_transform = transforms.Compose([\n",
        "    transforms.Resize((512, 1024)),  # H x W\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "mask_transform = transforms.Compose([\n",
        "    transforms.Resize((512, 1024), interpolation=Image.NEAREST),\n",
        "    transforms.PILToTensor()\n",
        "])\n",
        "\n",
        "# ✅ GTA5 Training Dataset with Augmentation ON\n",
        "gta5_dataset = GTA5Dataset(\n",
        "    root='/content/datasets/GTA5',\n",
        "    transform=image_transform,\n",
        "    target_transform=mask_transform,\n",
        "    augment=True  # Augmentation Step 1: Horizontal Flip\n",
        ")\n",
        "\n",
        "# ✅ Cityscapes Validation Dataset\n",
        "class CityscapesDataset(Dataset):\n",
        "    def __init__(self, root_dir, split='val', transform=None, target_transform=None):\n",
        "        self.image_dir = os.path.join(root_dir, \"leftImg8bit\", split)\n",
        "        self.label_dir = os.path.join(root_dir, \"gtFine\", split)\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "        self.images = []\n",
        "        self.labels = []\n",
        "        for city in os.listdir(self.image_dir):\n",
        "            img_folder = os.path.join(self.image_dir, city)\n",
        "            label_folder = os.path.join(self.label_dir, city)\n",
        "            for file_name in os.listdir(img_folder):\n",
        "                if file_name.endswith(\"_leftImg8bit.png\"):\n",
        "                    base = file_name.replace(\"_leftImg8bit.png\", \"\")\n",
        "                    img_path = os.path.join(img_folder, file_name)\n",
        "                    label_path = os.path.join(label_folder, base + \"_gtFine_labelTrainIds.png\")\n",
        "                    if os.path.exists(label_path):\n",
        "                        self.images.append(img_path)\n",
        "                        self.labels.append(label_path)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.images[idx]).convert(\"RGB\")\n",
        "        mask = Image.open(self.labels[idx])\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        if self.target_transform:\n",
        "            mask = self.target_transform(mask).long()\n",
        "        return img, mask\n",
        "\n",
        "val_dataset = CityscapesDataset(\n",
        "    root_dir='/content/datasets/Cityscapes',\n",
        "    transform=image_transform,\n",
        "    target_transform=mask_transform\n",
        ")\n",
        "\n",
        "# ✅ DataLoaders\n",
        "train_loader = DataLoader(gta5_dataset, batch_size=2, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "print(f\"✅ GTA5 dataset size: {len(gta5_dataset)} | Cityscapes val size: {len(val_dataset)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0juclluldKy",
        "outputId": "10a87f8c-efe9-4013-a93b-3c20b91725fd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ GTA5 dataset size: 2500 | Cityscapes val size: 500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the dataloaders"
      ],
      "metadata": {
        "id": "vmqBHOpypDVC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# ✅ Dataloaders\n",
        "train_loader = DataLoader(gta5_dataset, batch_size=2, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "print(f\"✅ Dataloaders ready | Train: {len(train_loader)} batches | Val: {len(val_loader)} batches\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zJOTBeXpJuj",
        "outputId": "ece45d0c-8526-43d3-9bc1-3f6d572be3d7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Dataloaders ready | Train: 1250 batches | Val: 250 batches\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Clone the official project repo\n",
        "!git clone https://github.com/Gabrysse/MLDL2024_project1.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sx4XnamppdO",
        "outputId": "4c69a9e3-dbeb-439b-a200-25ce90faa4a0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MLDL2024_project1'...\n",
            "remote: Enumerating objects: 34, done.\u001b[K\n",
            "remote: Counting objects: 100% (19/19), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 34 (delta 9), reused 3 (delta 3), pack-reused 15 (from 1)\u001b[K\n",
            "Receiving objects: 100% (34/34), 11.29 KiB | 11.29 MiB/s, done.\n",
            "Resolving deltas: 100% (9/9), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/MLDL2024_project1')\n",
        "\n",
        "from models.bisenet.build_bisenet import BiSeNet\n",
        "import torch\n",
        "\n",
        "# ✅ Load BiSeNet with ResNet18 and move to device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BiSeNet(num_classes=19, context_path='resnet18')\n",
        "model = model.to(device)\n",
        "\n",
        "print(\"✅ BiSeNet with ResNet18 loaded and moved to\", device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMjnq1YTpj-l",
        "outputId": "9346dd8e-8fc8-49be-9c78-50d94f8f6e3f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 207MB/s]\n",
            "Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n",
            "100%|██████████| 171M/171M [00:00<00:00, 206MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ BiSeNet with ResNet18 loaded and moved to cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the Loss, Optimizer, and AMP Scaler"
      ],
      "metadata": {
        "id": "GPhELNvxqD-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.cuda.amp import GradScaler\n",
        "\n",
        "# ✅ Loss function — ignore label 255\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
        "\n",
        "# ✅ Optimizer (SGD)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
        "\n",
        "# ✅ Mixed precision scaler for AMP\n",
        "scaler = GradScaler()\n",
        "\n",
        "print(\"✅ Loss, optimizer, and AMP scaler initialized.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pufceFKkqFqY",
        "outputId": "5e97dc71-217e-4194-eabf-160f8b36f33f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Loss, optimizer, and AMP scaler initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training (Augmentation 1 — Horizontal Flip)"
      ],
      "metadata": {
        "id": "5eubIeWJrlka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.cuda.amp import autocast\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "import os\n",
        "\n",
        "epochs = 50\n",
        "best_val_loss = float('inf')\n",
        "save_path = \"/content/drive/MyDrive/Semantic_Segmentation/bisenet_gta5_aug1.pth\"\n",
        "\n",
        "print(\"🟢 Starting BiSeNet training with Horizontal Flip Augmentation...\")\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    loop = tqdm(train_loader, total=len(train_loader), desc=f\"Epoch [{epoch+1}/{epochs}]\", leave=False)\n",
        "\n",
        "    for images, targets in loop:\n",
        "        images = images.to(device)\n",
        "        targets = targets.squeeze(1).long().to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        with autocast():\n",
        "            output, aux1, aux2 = model(images)\n",
        "            loss1 = criterion(output, targets)\n",
        "            loss2 = criterion(aux1, targets)\n",
        "            loss3 = criterion(aux2, targets)\n",
        "            loss = loss1 + 0.4 * (loss2 + loss3)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        loop.set_postfix(loss=loss.item())\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "\n",
        "    # ✅ Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for val_imgs, val_masks in val_loader:\n",
        "            val_imgs = val_imgs.to(device)\n",
        "            val_masks = val_masks.squeeze(1).long().to(device)\n",
        "\n",
        "            with autocast():\n",
        "                val_out = model(val_imgs)\n",
        "                val_loss_batch = criterion(val_out, val_masks)\n",
        "\n",
        "            val_loss += val_loss_batch.item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "    # ✅ Save best model\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "        print(f\"💾 Best model saved at epoch {epoch+1} | Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    free_gpu = torch.cuda.mem_get_info()[0] / (1024 ** 3)\n",
        "\n",
        "    print(f\"✅ Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Free GPU: {free_gpu:.2f} GB\")\n",
        "\n",
        "print(\"🏁 Augmented training complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0EzfLOHrmMy",
        "outputId": "363fb4ed-9d06-437f-af66-ff56f0890d8d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🟢 Starting BiSeNet training with Horizontal Flip Augmentation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Best model saved at epoch 1 | Val Loss: 3.1585\n",
            "✅ Epoch 1 | Train Loss: 1.3188 | Val Loss: 3.1585 | Free GPU: 14.09 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 2 | Train Loss: 0.9177 | Val Loss: 3.6551 | Free GPU: 14.05 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Best model saved at epoch 3 | Val Loss: 3.0835\n",
            "✅ Epoch 3 | Train Loss: 0.8001 | Val Loss: 3.0835 | Free GPU: 14.14 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Best model saved at epoch 4 | Val Loss: 2.7473\n",
            "✅ Epoch 4 | Train Loss: 0.7278 | Val Loss: 2.7473 | Free GPU: 14.08 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 5 | Train Loss: 0.6896 | Val Loss: 3.1206 | Free GPU: 14.01 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 6 | Train Loss: 0.6549 | Val Loss: 3.2686 | Free GPU: 13.89 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 7 | Train Loss: 0.5988 | Val Loss: 4.4595 | Free GPU: 13.95 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 8 | Train Loss: 0.5777 | Val Loss: 3.8104 | Free GPU: 14.08 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 9 | Train Loss: 0.5488 | Val Loss: 5.2375 | Free GPU: 14.07 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 10 | Train Loss: 0.7828 | Val Loss: 3.7564 | Free GPU: 13.99 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 11 | Train Loss: 0.5973 | Val Loss: 3.6777 | Free GPU: 13.96 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 12 | Train Loss: 0.5366 | Val Loss: 4.8938 | Free GPU: 14.05 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 13 | Train Loss: 0.5303 | Val Loss: 3.3479 | Free GPU: 14.01 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 14 | Train Loss: 0.4934 | Val Loss: 4.0415 | Free GPU: 13.94 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 15 | Train Loss: 0.4687 | Val Loss: 3.9914 | Free GPU: 14.05 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 16 | Train Loss: 0.4518 | Val Loss: 4.4608 | Free GPU: 14.09 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 17 | Train Loss: 0.4376 | Val Loss: 5.4620 | Free GPU: 14.07 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 18 | Train Loss: 0.4280 | Val Loss: 5.6254 | Free GPU: 14.03 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 19 | Train Loss: 0.4543 | Val Loss: 6.7120 | Free GPU: 14.03 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 20 | Train Loss: 0.4032 | Val Loss: 4.8725 | Free GPU: 13.93 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 21 | Train Loss: 0.3928 | Val Loss: 7.4521 | Free GPU: 14.02 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 22 | Train Loss: 0.3904 | Val Loss: 4.5124 | Free GPU: 14.12 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 23 | Train Loss: 0.3992 | Val Loss: 5.1738 | Free GPU: 14.04 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 24 | Train Loss: 0.3727 | Val Loss: 4.2011 | Free GPU: 14.11 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 25 | Train Loss: 0.3612 | Val Loss: 4.5690 | Free GPU: 14.07 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 26 | Train Loss: 0.3498 | Val Loss: 5.2068 | Free GPU: 14.02 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 27 | Train Loss: 0.3419 | Val Loss: 4.9231 | Free GPU: 14.06 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 28 | Train Loss: 0.3325 | Val Loss: 5.3270 | Free GPU: 14.07 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 29 | Train Loss: 0.3250 | Val Loss: 4.9339 | Free GPU: 14.06 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 30 | Train Loss: 0.3253 | Val Loss: 5.3346 | Free GPU: 13.97 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 31 | Train Loss: 0.3242 | Val Loss: 5.4281 | Free GPU: 14.03 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 32 | Train Loss: 0.3264 | Val Loss: 6.6278 | Free GPU: 14.11 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 33 | Train Loss: 0.3084 | Val Loss: 6.8159 | Free GPU: 14.00 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 34 | Train Loss: 0.3127 | Val Loss: 5.5634 | Free GPU: 13.98 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 35 | Train Loss: 0.3043 | Val Loss: 4.2039 | Free GPU: 14.04 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 36 | Train Loss: 0.3086 | Val Loss: 4.2590 | Free GPU: 13.99 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 37 | Train Loss: 0.2906 | Val Loss: 4.4253 | Free GPU: 14.09 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 38 | Train Loss: 0.2912 | Val Loss: 6.0295 | Free GPU: 14.10 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 39 | Train Loss: 0.2843 | Val Loss: 4.8113 | Free GPU: 14.04 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 40 | Train Loss: 0.2990 | Val Loss: 5.2873 | Free GPU: 14.05 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 41 | Train Loss: 0.2851 | Val Loss: 4.0306 | Free GPU: 14.08 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 42 | Train Loss: 0.2763 | Val Loss: 5.0709 | Free GPU: 13.98 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 43 | Train Loss: 0.2700 | Val Loss: 4.7329 | Free GPU: 14.09 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 44 | Train Loss: 0.2665 | Val Loss: 4.2859 | Free GPU: 14.11 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 45 | Train Loss: 0.2875 | Val Loss: 4.8495 | Free GPU: 14.07 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 46 | Train Loss: 0.2698 | Val Loss: 4.4937 | Free GPU: 14.02 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 47 | Train Loss: 0.2687 | Val Loss: 5.1597 | Free GPU: 14.01 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 48 | Train Loss: 0.2614 | Val Loss: 4.7317 | Free GPU: 14.04 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 49 | Train Loss: 0.3066 | Val Loss: 5.2422 | Free GPU: 14.05 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 50 | Train Loss: 0.3530 | Val Loss: 5.0233 | Free GPU: 14.04 GB\n",
            "🏁 Augmented training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-XHkAXPSUPOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Save augmented model to Google Drive\n",
        "aug_model_path = \"/content/drive/MyDrive/Semantic_Segmentation/bisenet_gta5_augmented.pth\"\n",
        "torch.save(model.state_dict(), aug_model_path)\n",
        "print(f\"✅ Augmented model saved to: {aug_model_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZShoLYhUO9v",
        "outputId": "d7a3108a-d828-42a0-cbbb-fb44f69a6a10"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Augmented model saved to: /content/drive/MyDrive/Semantic_Segmentation/bisenet_gta5_augmented.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from models.bisenet.build_bisenet import BiSeNet\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "# ✅ Reload model and set to eval mode\n",
        "model = BiSeNet(num_classes=19, context_path='resnet18')\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/Semantic_Segmentation/bisenet_gta5_augmented.pth\"))\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# ✅ Metric computation\n",
        "def compute_miou(preds, labels, num_classes=19):\n",
        "    ious = []\n",
        "    preds = preds.detach().cpu().numpy()\n",
        "    labels = labels.detach().cpu().numpy()\n",
        "\n",
        "    for cls in range(num_classes):\n",
        "        pred_inds = preds == cls\n",
        "        target_inds = labels == cls\n",
        "        intersection = (pred_inds & target_inds).sum()\n",
        "        union = (pred_inds | target_inds).sum()\n",
        "        if union == 0:\n",
        "            ious.append(float('nan'))  # class not present\n",
        "        else:\n",
        "            ious.append(intersection / union)\n",
        "    return np.array(ious)\n",
        "\n",
        "# ✅ Evaluate on validation set\n",
        "ious = []\n",
        "print(\"🔍 Evaluating mIoU...\")\n",
        "for images, masks in tqdm(val_loader, total=len(val_loader)):\n",
        "    images = images.to(device)\n",
        "    masks = masks.squeeze(1).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(images)\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        iou = compute_miou(preds, masks)\n",
        "        ious.append(iou)\n",
        "\n",
        "# ✅ Compute final mIoU\n",
        "ious = np.stack(ious)\n",
        "mean_iou = np.nanmean(ious)\n",
        "print(f\"📊 Final mIoU with augmentation (GTA5 → Cityscapes): {mean_iou:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCRN8LNCWPIh",
        "outputId": "6bc4ca36-f442-4a06-caa8-e6f39e5772a8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Evaluating mIoU...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 250/250 [00:29<00:00,  8.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 Final mIoU with augmentation (GTA5 → Cityscapes): 0.0524\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "NUM_CLASSES = 19\n",
        "conf_matrix = np.zeros((NUM_CLASSES, NUM_CLASSES), dtype=np.int64)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for images, targets in tqdm(val_loader, desc=\"🔍 Evaluating per-class IoU\"):\n",
        "        images = images.to(device)\n",
        "        targets = targets.squeeze(1).to(device)\n",
        "\n",
        "        preds = torch.argmax(model(images), dim=1)\n",
        "\n",
        "        for pred, target in zip(preds, targets):\n",
        "            mask = (target >= 0) & (target < NUM_CLASSES)\n",
        "            hist = np.bincount(\n",
        "                NUM_CLASSES * target[mask].cpu().numpy() + pred[mask].cpu().numpy(),\n",
        "                minlength=NUM_CLASSES ** 2\n",
        "            ).reshape(NUM_CLASSES, NUM_CLASSES)\n",
        "            conf_matrix += hist\n",
        "\n",
        "# Compute per-class IoU\n",
        "intersection = np.diag(conf_matrix)\n",
        "union = conf_matrix.sum(1) + conf_matrix.sum(0) - intersection\n",
        "iou = intersection / np.maximum(union, 1)\n",
        "\n",
        "labels = [\n",
        "    'road', 'sidewalk', 'building', 'wall', 'fence', 'pole', 'traffic light',\n",
        "    'traffic sign', 'vegetation', 'terrain', 'sky', 'person', 'rider', 'car',\n",
        "    'truck', 'bus', 'train', 'motorcycle', 'bicycle'\n",
        "]\n",
        "\n",
        "for name, score in zip(labels, iou):\n",
        "    print(f\"{name:<15}: {score:.4f}\")\n",
        "\n",
        "print(f\"\\n📊 Final mIoU with augmentation: {iou.mean():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjzB_psGL_DK",
        "outputId": "dc644c23-809b-492c-9dec-a5d92613a8d4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "🔍 Evaluating per-class IoU: 100%|██████████| 250/250 [00:29<00:00,  8.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "road           : 0.0061\n",
            "sidewalk       : 0.0321\n",
            "building       : 0.5654\n",
            "wall           : 0.0534\n",
            "fence          : 0.0159\n",
            "pole           : 0.0012\n",
            "traffic light  : 0.0088\n",
            "traffic sign   : 0.0028\n",
            "vegetation     : 0.0000\n",
            "terrain        : 0.0038\n",
            "sky            : 0.0012\n",
            "person         : 0.0009\n",
            "rider          : 0.0455\n",
            "car            : 0.2463\n",
            "truck          : 0.0087\n",
            "bus            : 0.0000\n",
            "train          : 0.0154\n",
            "motorcycle     : 0.0101\n",
            "bicycle        : 0.0000\n",
            "\n",
            "📊 Final mIoU with augmentation: 0.0536\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}