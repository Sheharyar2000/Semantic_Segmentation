{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooqMsbKGcoXR",
        "outputId": "234fe286-8353-4b77-b9a7-c0e2b30556fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrcbFutCc3Nu",
        "outputId": "f15a6941-9512-4413-e348-3581a2248afd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Both datasets extracted.\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Define paths\n",
        "gta5_zip = \"/content/drive/MyDrive/Semantic_Segmentation/GTA5.zip\"\n",
        "cityscapes_zip = \"/content/drive/MyDrive/Semantic_Segmentation/Cityscapes.zip\"\n",
        "\n",
        "os.makedirs(\"/content/datasets/GTA5\", exist_ok=True)\n",
        "os.makedirs(\"/content/datasets/Cityscapes\", exist_ok=True)\n",
        "\n",
        "# Unzip GTA5\n",
        "with zipfile.ZipFile(gta5_zip, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"/content/datasets/GTA5\")\n",
        "\n",
        "# Unzip Cityscapes\n",
        "with zipfile.ZipFile(cityscapes_zip, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"/content/datasets/Cityscapes\")\n",
        "\n",
        "print(\"âœ… Both datasets extracted.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaMvuyg8du08",
        "outputId": "5fc65ecf-1f6d-44c5-91bc-19162b1fad93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Fixed GTA5 folder structure\n",
            "âœ… Fixed Cityscapes folder structure\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Fix GTA5\n",
        "if os.path.exists(\"/content/datasets/GTA5/GTA5\"):\n",
        "    shutil.move(\"/content/datasets/GTA5/GTA5/images\", \"/content/datasets/GTA5/images\")\n",
        "    shutil.move(\"/content/datasets/GTA5/GTA5/labels\", \"/content/datasets/GTA5/labels\")\n",
        "    shutil.rmtree(\"/content/datasets/GTA5/GTA5\")\n",
        "    print(\"âœ… Fixed GTA5 folder structure\")\n",
        "\n",
        "# Fix Cityscapes\n",
        "nested_city = \"/content/datasets/Cityscapes/Cityscapes/Cityspaces\"\n",
        "if os.path.exists(nested_city):\n",
        "    shutil.move(os.path.join(nested_city, \"images\"), \"/content/datasets/Cityscapes/leftImg8bit\")\n",
        "    shutil.move(os.path.join(nested_city, \"gtFine\"), \"/content/datasets/Cityscapes/gtFine\")\n",
        "    shutil.rmtree(\"/content/datasets/Cityscapes/Cityscapes\")\n",
        "    print(\"âœ… Fixed Cityscapes folder structure\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lbc_WORodygD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# âœ… Label mapping (GTA5 to Cityscapes IDs)\n",
        "GTA5_TO_CITYSCAPES = {\n",
        "    7: 0, 8: 1, 11: 2, 12: 3, 13: 4, 15: 5, 17: 6, 19: 7, 20: 8,\n",
        "    21: 9, 22: 10, 23: 11, 24: 12, 26: 13, 27: 14, 28: 15,\n",
        "    31: 16, 32: 17, 33: 18\n",
        "}\n",
        "\n",
        "class GTA5Dataset(Dataset):\n",
        "    def __init__(self, root, transform=None, target_transform=None):\n",
        "        self.image_dir = os.path.join(root, \"images\")\n",
        "        self.label_dir = os.path.join(root, \"labels\")\n",
        "        self.images = sorted(os.listdir(self.image_dir))\n",
        "        self.labels = sorted(os.listdir(self.label_dir))\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def encode_labels(self, mask):\n",
        "        remapped = np.full_like(mask, 255)\n",
        "        for gta_id, city_id in GTA5_TO_CITYSCAPES.items():\n",
        "            remapped[mask == gta_id] = city_id\n",
        "        return remapped\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(os.path.join(self.image_dir, self.images[idx])).convert(\"RGB\")\n",
        "        mask = Image.open(os.path.join(self.label_dir, self.labels[idx]))\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        if self.target_transform:\n",
        "            mask = self.target_transform(mask)\n",
        "            mask = self.encode_labels(mask.squeeze().numpy())\n",
        "            mask = torch.from_numpy(mask).long().unsqueeze(0)\n",
        "\n",
        "        return img, mask\n",
        "\n",
        "class CityscapesDataset(Dataset):\n",
        "    def __init__(self, root, split='val', transform=None, target_transform=None):\n",
        "        self.image_dir = os.path.join(root, \"leftImg8bit\", split)\n",
        "        self.label_dir = os.path.join(root, \"gtFine\", split)\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "        self.images = []\n",
        "        self.labels = []\n",
        "\n",
        "        for city in os.listdir(self.image_dir):\n",
        "            img_folder = os.path.join(self.image_dir, city)\n",
        "            label_folder = os.path.join(self.label_dir, city)\n",
        "\n",
        "            for file_name in os.listdir(img_folder):\n",
        "                if file_name.endswith(\"_leftImg8bit.png\"):\n",
        "                    base = file_name.replace(\"_leftImg8bit.png\", \"\")\n",
        "                    self.images.append(os.path.join(img_folder, file_name))\n",
        "                    self.labels.append(os.path.join(label_folder, base + \"_gtFine_labelTrainIds.png\"))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.images[idx]).convert(\"RGB\")\n",
        "        mask = Image.open(self.labels[idx])\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        if self.target_transform:\n",
        "            mask = self.target_transform(mask)\n",
        "\n",
        "        return img, mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lkr-G__Fd4HU",
        "outputId": "b66b9150-35be-46a9-c381-519c814507b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Train loader: 1250 batches | Val loader: 250 batches\n"
          ]
        }
      ],
      "source": [
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Image transformation (standard normalization for pretrained ImageNet models)\n",
        "image_transform = transforms.Compose([\n",
        "    transforms.Resize((720, 1280)),  # Match GTA5 native resolution\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Target label transformation (no normalization)\n",
        "target_transform = transforms.Compose([\n",
        "    transforms.Resize((720, 1280), interpolation=Image.NEAREST),\n",
        "    transforms.PILToTensor(),\n",
        "])\n",
        "\n",
        "# Dataset paths\n",
        "gta_path = \"/content/datasets/GTA5\"\n",
        "cityscapes_path = \"/content/datasets/Cityscapes\"\n",
        "\n",
        "# Initialize datasets\n",
        "gta_dataset = GTA5Dataset(gta_path, transform=image_transform, target_transform=target_transform)\n",
        "cityscapes_val_dataset = CityscapesDataset(cityscapes_path, split='val', transform=image_transform, target_transform=target_transform)\n",
        "\n",
        "# Dataloaders\n",
        "train_loader = DataLoader(gta_dataset, batch_size=2, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(cityscapes_val_dataset, batch_size=2, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "print(f\"âœ… Train loader: {len(train_loader)} batches | Val loader: {len(val_loader)} batches\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQr8Odr4d8Mt",
        "outputId": "2d856d59-b03a-4424-c56e-71944b273b32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MLDL2024_project1'...\n",
            "remote: Enumerating objects: 34, done.\u001b[K\n",
            "remote: Counting objects: 100% (21/21), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 34 (delta 9), reused 3 (delta 3), pack-reused 13 (from 1)\u001b[K\n",
            "Receiving objects: 100% (34/34), 11.29 KiB | 428.00 KiB/s, done.\n",
            "Resolving deltas: 100% (9/9), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Gabrysse/MLDL2024_project1.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmXMACvkd9Yk",
        "outputId": "c62da768-3927-4d63-94c0-26c2241eaa87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… BiSeNet imported successfully.\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/MLDL2024_project1\")\n",
        "\n",
        "from models.bisenet.build_bisenet import BiSeNet\n",
        "print(\"âœ… BiSeNet imported successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3hNjZfGd_St"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "def extract_amplitude_phase(img_np):\n",
        "    img_np = np.transpose(img_np, (2, 0, 1))\n",
        "    fft = np.fft.fft2(img_np, axes=(-2, -1))\n",
        "    amplitude = np.abs(fft)\n",
        "    phase = np.angle(fft)\n",
        "    return amplitude, phase\n",
        "\n",
        "def FDA_source_to_target(src_img, tgt_img, L=0.01):\n",
        "    src_np = np.asarray(src_img, dtype=np.float32)\n",
        "    tgt_np = np.asarray(tgt_img, dtype=np.float32)\n",
        "\n",
        "    # Normalize\n",
        "    src_np /= 255.0\n",
        "    tgt_np /= 255.0\n",
        "\n",
        "    src_amp, src_phase = extract_amplitude_phase(src_np)\n",
        "    tgt_amp, _ = extract_amplitude_phase(tgt_np)\n",
        "\n",
        "    # Swap low-frequency amplitudes\n",
        "    _, h, w = src_amp.shape\n",
        "    b = int(np.floor(min(h, w) * L))\n",
        "    c_h = int(np.floor(h / 2.0))\n",
        "    c_w = int(np.floor(w / 2.0))\n",
        "\n",
        "    h1 = c_h - b\n",
        "    h2 = c_h + b + 1\n",
        "    w1 = c_w - b\n",
        "    w2 = c_w + b + 1\n",
        "\n",
        "    src_amp[:, h1:h2, w1:w2] = tgt_amp[:, h1:h2, w1:w2]\n",
        "\n",
        "    # Reconstruct back to image\n",
        "    fft_src = src_amp * np.exp(1j * src_phase)\n",
        "    src_inversed = np.fft.ifft2(fft_src, axes=(-2, -1))\n",
        "    src_inversed = np.real(src_inversed)\n",
        "\n",
        "    src_inversed = np.transpose(src_inversed, (1, 2, 0))\n",
        "    src_inversed = np.clip(src_inversed * 255.0, 0, 255).astype(np.uint8)\n",
        "\n",
        "    return Image.fromarray(src_inversed)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iPeStVZueGMl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Label remapping\n",
        "GTA5_TO_CITYSCAPES = {\n",
        "    7: 0, 8: 1, 11: 2, 12: 3, 13: 4, 15: 5, 17: 6, 19: 7, 20: 8,\n",
        "    21: 9, 22: 10, 23: 11, 24: 12, 26: 13, 27: 14, 28: 15,\n",
        "    31: 16, 32: 17, 33: 18\n",
        "}\n",
        "\n",
        "class GTA5FDA(Dataset):\n",
        "    def __init__(self, gta5_root, cityscapes_root, transform=None, target_transform=None):\n",
        "        self.image_dir = os.path.join(gta5_root, \"images\")\n",
        "        self.label_dir = os.path.join(gta5_root, \"labels\")\n",
        "        self.cs_image_dir = os.path.join(cityscapes_root, \"leftImg8bit\", \"train\")\n",
        "\n",
        "        self.images = sorted(os.listdir(self.image_dir))\n",
        "        self.labels = sorted(os.listdir(self.label_dir))\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "        self.cityscapes_images = []\n",
        "        for city in os.listdir(self.cs_image_dir):\n",
        "            img_folder = os.path.join(self.cs_image_dir, city)\n",
        "            for file_name in os.listdir(img_folder):\n",
        "                if file_name.endswith(\"_leftImg8bit.png\"):\n",
        "                    self.cityscapes_images.append(os.path.join(img_folder, file_name))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def encode_labels(self, mask):\n",
        "        remapped = np.full_like(mask, 255)\n",
        "        for gta_id, city_id in GTA5_TO_CITYSCAPES.items():\n",
        "            remapped[mask == gta_id] = city_id\n",
        "        return remapped\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.image_dir, self.images[idx])\n",
        "        label_path = os.path.join(self.label_dir, self.labels[idx])\n",
        "\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        label = Image.open(label_path)\n",
        "\n",
        "        # Apply FDA with a random Cityscapes image\n",
        "        tgt_img_path = random.choice(self.cityscapes_images)\n",
        "        tgt_img = Image.open(tgt_img_path).convert(\"RGB\")\n",
        "        img = FDA_source_to_target(img, tgt_img, L=0.01)\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "            label = self.encode_labels(label.squeeze().numpy())\n",
        "            label = torch.from_numpy(label).long().unsqueeze(0)\n",
        "\n",
        "        return img, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W814PKIReJPt",
        "outputId": "44afc47a-4a65-4ace-b467-742ecc4451b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Train loader: 1250 batches | Val loader: 250 batches\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Image and label transforms\n",
        "image_transform = transforms.Compose([\n",
        "    transforms.Resize((512, 1024)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "label_transform = transforms.Compose([\n",
        "    transforms.Resize((512, 1024), interpolation=Image.NEAREST),\n",
        "    transforms.PILToTensor()\n",
        "])\n",
        "\n",
        "#  Initialize FDA-augmented GTA5 training dataset\n",
        "fda_dataset = GTA5FDA(\n",
        "    gta5_root='/content/datasets/GTA5',\n",
        "    cityscapes_root='/content/datasets/Cityscapes',\n",
        "    transform=image_transform,\n",
        "    target_transform=label_transform\n",
        ")\n",
        "\n",
        "#  Validation dataset (Cityscapes)\n",
        "val_dataset = CityscapesDataset(\n",
        "    root='/content/datasets/Cityscapes',\n",
        "    split='val',\n",
        "    transform=image_transform,\n",
        "    target_transform=label_transform\n",
        ")\n",
        "\n",
        "#  Dataloaders\n",
        "train_loader = DataLoader(fda_dataset, batch_size=2, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, num_workers=2)\n",
        "\n",
        "print(\"âœ… Train loader:\", len(train_loader), \"batches | Val loader:\", len(val_loader), \"batches\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IP1CPLOeMeO",
        "outputId": "261ab077-e9dd-4d5f-ad90-f3af744788ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44.7M/44.7M [00:00<00:00, 140MB/s]\n",
            "Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 171M/171M [00:01<00:00, 105MB/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… BiSeNet model initialized and moved to cpu\n"
          ]
        }
      ],
      "source": [
        "from models.bisenet.build_bisenet import BiSeNet\n",
        "import torch\n",
        "\n",
        "#  Dynamic device assignment\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#  Initialize BiSeNet with ResNet-18 backbone\n",
        "model = BiSeNet(num_classes=19, context_path='resnet18')\n",
        "model = model.to(device)\n",
        "\n",
        "print(f\"âœ… BiSeNet model initialized and moved to {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8ibguhleN3-",
        "outputId": "dd0dbc93-c952-4cb1-ed3c-6841ff674490"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Loss, optimizer, and AMP scaler initialized.\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.cuda.amp import GradScaler\n",
        "\n",
        "# Loss function (ignore index 255 for unlabeled/void class)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
        "\n",
        "# AMP scaler\n",
        "scaler = GradScaler()\n",
        "\n",
        "print(\" Loss, optimizer, and AMP scaler initialized.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "As9wA6fcc2N8",
        "outputId": "46ae400e-64b2-4ced-89da-690f83dede22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ” Resumed from /content/drive/MyDrive/Semantic_Segmentation/checkpoints_fda/bisenet_gta5_fda_epoch_49.pth | Starting at epoch 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ðŸŸ¢ Epoch 50 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1250/1250 [5:19:28<00:00, 15.34s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Checkpoint saved: /content/drive/MyDrive/Semantic_Segmentation/checkpoints_fda/bisenet_gta5_fda_epoch_50.pth\n",
            "ðŸ“Š Epoch 50 | Train Loss: 0.2605 | Val Loss: 5.3436\n",
            "FDA-based training complete.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from tqdm import tqdm\n",
        "from models.bisenet.build_bisenet import BiSeNet\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Paths\n",
        "checkpoint_dir = \"/content/drive/MyDrive/Semantic_Segmentation/checkpoints_fda\"\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "final_model_path = \"/content/drive/MyDrive/Semantic_Segmentation/bisenet_gta5_fda_final.pth\"\n",
        "\n",
        "# Initialize model\n",
        "model = BiSeNet(num_classes=19, context_path='resnet18').to(device)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
        "scaler = GradScaler()\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "start_epoch = 1\n",
        "num_epochs = 50\n",
        "\n",
        "# Resume from checkpoint if exists\n",
        "import re\n",
        "\n",
        "# Extract numerical epoch index safely and sort accordingly\n",
        "def extract_epoch(file_name):\n",
        "    match = re.search(r'epoch_(\\d+)\\.pth', file_name)\n",
        "    return int(match.group(1)) if match else -1\n",
        "\n",
        "latest_ckpts = sorted(\n",
        "    [f for f in os.listdir(checkpoint_dir) if f.endswith('.pth')],\n",
        "    key=extract_epoch\n",
        ")\n",
        "\n",
        "if latest_ckpts:\n",
        "    latest = os.path.join(checkpoint_dir, latest_ckpts[-1])\n",
        "    checkpoint = torch.load(latest, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
        "    # Check if scaler_state is not empty before loading, and only load once\n",
        "    if checkpoint.get('scaler_state') and checkpoint['scaler_state']:\n",
        "        scaler.load_state_dict(checkpoint['scaler_state'])\n",
        "\n",
        "    best_val_loss = checkpoint['best_val_loss']\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "    print(f\"ðŸ” Resumed from {latest} | Starting at epoch {start_epoch}\")\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(start_epoch, num_epochs + 1):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for images, masks in tqdm(train_loader, desc=f\"ðŸŸ¢ Epoch {epoch} Training\"):\n",
        "        images = images.to(device)\n",
        "        masks = masks.squeeze(1).to(device).long()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        with autocast():\n",
        "            out, aux2, aux3 = model(images)\n",
        "            loss = (criterion(out, masks) +\n",
        "                    0.4 * criterion(aux2, masks) +\n",
        "                    0.4 * criterion(aux3, masks))\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "    # âœ… Validation Phase (inside epoch loop)\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for val_imgs, val_masks in val_loader:\n",
        "            val_imgs = val_imgs.to(device)\n",
        "            val_masks = val_masks.squeeze(1).to(device).long()\n",
        "\n",
        "            with autocast():\n",
        "                val_out = model(val_imgs)\n",
        "\n",
        "            if val_out.shape[2:] != val_masks.shape[1:]:\n",
        "                val_out = torch.nn.functional.interpolate(val_out, size=val_masks.shape[1:], mode='bilinear', align_corners=False)\n",
        "\n",
        "            val_loss = criterion(val_out, val_masks)\n",
        "            total_val_loss += val_loss.item()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(val_loader)\n",
        "\n",
        "    #  Save best model\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        torch.save(model.state_dict(), final_model_path)\n",
        "        print(f\"ðŸ’¾ Best model saved at epoch {epoch} | Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    #  Save checkpoint every epoch\n",
        "    ckpt_path = os.path.join(checkpoint_dir, f\"bisenet_gta5_fda_epoch_{epoch}.pth\")\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state': model.state_dict(),\n",
        "        'optimizer_state': optimizer.state_dict(),\n",
        "        'scaler_state': scaler.state_dict(), # Always save scaler state\n",
        "        'best_val_loss': best_val_loss\n",
        "    }\n",
        "    torch.save(checkpoint, ckpt_path)\n",
        "    print(f\"âœ… Checkpoint saved: {ckpt_path}\")\n",
        "    print(f\"ðŸ“Š Epoch {epoch} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "print(\"FDA-based training complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "from models.bisenet.build_bisenet import BiSeNet\n",
        "\n",
        "# Configuration\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "num_classes = 19\n",
        "checkpoint_path = \"/content/drive/MyDrive/Semantic_Segmentation/bisenet_gta5_fda_final.pth\"\n",
        "\n",
        "# Load model\n",
        "model = BiSeNet(num_classes=num_classes, context_path='resnet18')\n",
        "model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Evaluation Function (per-class IoU + mIoU)\n",
        "def evaluate_miou(model, dataloader, num_classes=19):\n",
        "    hist = torch.zeros(num_classes, num_classes).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(dataloader, desc=\"ðŸ” Evaluating mIoU\"):\n",
        "            images = images.to(device)\n",
        "            labels = labels.squeeze(1).to(device)  # [B, H, W]\n",
        "\n",
        "            preds = model(images)\n",
        "            if isinstance(preds, tuple):  # Handle multi-output model\n",
        "                preds = preds[0]\n",
        "            preds = torch.argmax(preds, dim=1)  # [B, H, W]\n",
        "\n",
        "            for p, t in zip(preds, labels):\n",
        "                # Flatten predictions and labels\n",
        "                p = p.view(-1)\n",
        "                t = t.view(-1)\n",
        "                mask = (t >= 0) & (t < num_classes)\n",
        "                p = p[mask]\n",
        "                t = t[mask]\n",
        "\n",
        "                # Confusion matrix accumulation\n",
        "                hist += torch.bincount(num_classes * t + p, minlength=num_classes**2).reshape(num_classes, num_classes)\n",
        "\n",
        "    # Compute per-class IoU and mIoU\n",
        "    iou = hist.diag() / (hist.sum(1) + hist.sum(0) - hist.diag() + 1e-6)\n",
        "    for i, val in enumerate(iou):\n",
        "        print(f\"Class {i:02d}: IoU = {val:.4f}\")\n",
        "    print(f\"\\nðŸ“Š Final mIoU: {iou.mean():.4f}\")\n",
        "\n",
        "# Run evaluation\n",
        "evaluate_miou(model, val_loader, num_classes=num_classes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0MTR-wb9Hw9",
        "outputId": "e159dd22-3e2a-4edb-c014-17d62d9d41dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ðŸ” Evaluating mIoU: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [14:34<00:00,  3.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class 00: IoU = 0.4281\n",
            "Class 01: IoU = 0.1856\n",
            "Class 02: IoU = 0.5618\n",
            "Class 03: IoU = 0.0354\n",
            "Class 04: IoU = 0.0345\n",
            "Class 05: IoU = 0.0162\n",
            "Class 06: IoU = 0.0073\n",
            "Class 07: IoU = 0.0000\n",
            "Class 08: IoU = 0.0000\n",
            "Class 09: IoU = 0.0019\n",
            "Class 10: IoU = 0.0005\n",
            "Class 11: IoU = 0.0012\n",
            "Class 12: IoU = 0.0031\n",
            "Class 13: IoU = 0.3649\n",
            "Class 14: IoU = 0.0000\n",
            "Class 15: IoU = 0.0000\n",
            "Class 16: IoU = 0.0000\n",
            "Class 17: IoU = 0.0000\n",
            "Class 18: IoU = 0.0000\n",
            "\n",
            "ðŸ“Š Final mIoU: 0.0863\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}