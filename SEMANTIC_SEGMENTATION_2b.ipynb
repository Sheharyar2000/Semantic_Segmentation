{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#SEMANTIC SEGMENTATION 2b"
      ],
      "metadata": {
        "id": "WRctDISfUK-Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mounting Google drive"
      ],
      "metadata": {
        "id": "NnmWuekwUPJI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Xh4RytjrltT",
        "outputId": "dcef991c-bfd8-428c-c5ef-847cab4438b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting up the dataset"
      ],
      "metadata": {
        "id": "3uo0Ua_GUSjR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Path to zip in Drive\n",
        "zip_path = '/content/drive/MyDrive/Semantic_Segmentation/Cityscapes.zip'\n",
        "extract_path = '/content/datasets/Cityscapes'\n",
        "\n",
        "# Create extract directory\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "# Unzip\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"✅ Cityscapes dataset extracted to:\", extract_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rh6r61K_veJF",
        "outputId": "49a32490-c78b-47e7-81fc-c6818d1997f5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Cityscapes dataset extracted to: /content/datasets/Cityscapes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Paths\n",
        "broken_root = '/content/datasets/Cityscapes/Cityscapes/Cityspaces'\n",
        "target_root = '/content/datasets/Cityscapes'\n",
        "\n",
        "# Move gtFine and images up\n",
        "for folder in ['gtFine', 'images']:\n",
        "    src = os.path.join(broken_root, folder)\n",
        "    dst = os.path.join(target_root, folder)\n",
        "    if os.path.exists(src):\n",
        "        shutil.move(src, dst)\n",
        "\n",
        "# Rename 'images' to 'leftImg8bit' to match standard naming\n",
        "os.rename(os.path.join(target_root, 'images'),\n",
        "          os.path.join(target_root, 'leftImg8bit'))\n",
        "\n",
        "print(\"✅ Folder structure corrected.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8s9cpNPqv-8Q",
        "outputId": "207d0c6a-450c-4dda-ae37-5c4040e74ba5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Folder structure corrected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cloning the github repository"
      ],
      "metadata": {
        "id": "PtRaauF8UWJT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Gabrysse/MLDL2024_project1.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tM8g0QOpwC5p",
        "outputId": "93137ead-4597-4cb8-e924-98cf4a168836"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MLDL2024_project1'...\n",
            "remote: Enumerating objects: 34, done.\u001b[K\n",
            "remote: Counting objects: 100% (19/19), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 34 (delta 9), reused 3 (delta 3), pack-reused 15 (from 1)\u001b[K\n",
            "Receiving objects: 100% (34/34), 11.29 KiB | 11.29 MiB/s, done.\n",
            "Resolving deltas: 100% (9/9), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls MLDL2024_project1/models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcqTjTaUwHVJ",
        "outputId": "374eb349-033b-40ba-a815-062cc5a32770"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bisenet  deeplabv2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls MLDL2024_project1/models/bisenet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vw6i7zYhwJVp",
        "outputId": "f32d0fa7-3194-4fc1-e399-f2968414bd4f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "build_bisenet.py  build_contextpath.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import torch\n",
        "\n",
        "# Add project path to system\n",
        "sys.path.append(\"/content/MLDL2024_project1\")\n",
        "\n",
        "# Initialize device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"🟢 Using device:\", device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cn4Xv2w5wMe6",
        "outputId": "a2c10349-1bd5-4fe9-8328-5f06a765fa76"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🟢 Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from models.bisenet.build_bisenet import BiSeNet\n",
        "\n",
        "# Initialize BiSeNet with 19 semantic classes and 'resnet18' backbone\n",
        "model = BiSeNet(num_classes=19, context_path='resnet18')\n",
        "\n",
        "# Move model to CUDA\n",
        "model = model.to(device)\n",
        "print(\"✅ BiSeNet model loaded and moved to\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uckfW24TwQa6",
        "outputId": "dcde132e-ac33-4004-a378-b86afe9d565e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 173MB/s]\n",
            "Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n",
            "100%|██████████| 171M/171M [00:01<00:00, 142MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ BiSeNet model loaded and moved to cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Datasetting and Dataloading"
      ],
      "metadata": {
        "id": "pNsRjE3HUeWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "# ✅ Dataset root path\n",
        "root_dir = \"/content/datasets/Cityscapes\"\n",
        "images_base = os.path.join(root_dir, \"leftImg8bit\")\n",
        "masks_base = os.path.join(root_dir, \"gtFine\")\n",
        "\n",
        "# ✅ Cityscapes Custom Dataset\n",
        "class CityscapesDataset(Dataset):\n",
        "    def __init__(self, split='train', transform=None, target_transform=None):\n",
        "        self.split = split\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        self.images = []\n",
        "        self.masks = []\n",
        "\n",
        "        cities_path = os.path.join(images_base, split)\n",
        "        for city in os.listdir(cities_path):\n",
        "            img_dir = os.path.join(cities_path, city)\n",
        "            mask_dir = os.path.join(masks_base, split, city)\n",
        "\n",
        "            for file_name in os.listdir(img_dir):\n",
        "                if file_name.endswith(\"_leftImg8bit.png\"):\n",
        "                    base = file_name.replace(\"_leftImg8bit.png\", \"\")\n",
        "                    img_path = os.path.join(img_dir, file_name)\n",
        "                    mask_path = os.path.join(mask_dir, base + \"_gtFine_labelTrainIds.png\")\n",
        "                    if os.path.exists(mask_path):\n",
        "                        self.images.append(img_path)\n",
        "                        self.masks.append(mask_path)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.images[idx]).convert('RGB')\n",
        "        mask = Image.open(self.masks[idx])\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        if self.target_transform:\n",
        "            mask = self.target_transform(mask)\n",
        "\n",
        "        return img, mask\n",
        "\n",
        "# ✅ Transformations\n",
        "image_transform = transforms.Compose([\n",
        "    transforms.Resize((512, 1024)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "mask_transform = transforms.Compose([\n",
        "    transforms.Resize((512, 1024), interpolation=Image.NEAREST),\n",
        "    transforms.PILToTensor()\n",
        "])\n",
        "\n",
        "# ✅ Datasets & Loaders\n",
        "train_dataset = CityscapesDataset(split='train', transform=image_transform, target_transform=mask_transform)\n",
        "val_dataset = CityscapesDataset(split='val', transform=image_transform, target_transform=mask_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "print(f\"✅ Dataset loaded: {len(train_dataset)} training samples, {len(val_dataset)} validation samples\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLvZbuGmwYWb",
        "outputId": "ce411f95-794c-4b80-c80c-e16f7c6bb174"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Dataset loaded: 1572 training samples, 500 validation samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.amp import autocast, GradScaler\n",
        "\n",
        "# ✅ Combined loss for 3 outputs (main, aux1, aux2)\n",
        "class BiSeNetLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
        "\n",
        "    def forward(self, preds, target):\n",
        "        main, aux1, aux2 = preds\n",
        "        loss1 = self.criterion(main, target)\n",
        "        loss2 = self.criterion(aux1, target)\n",
        "        loss3 = self.criterion(aux2, target)\n",
        "        return loss1 + 0.4 * (loss2 + loss3)  # standard BiSeNet weighting\n",
        "\n",
        "# ✅ Initialize loss\n",
        "criterion = BiSeNetLoss()\n",
        "\n",
        "# ✅ Optimizer (SGD with momentum, same as 2a)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
        "\n",
        "# ✅ Mixed precision scaler\n",
        "scaler = GradScaler(device='cuda')\n",
        "\n",
        "print(\"✅ BiSeNet loss, optimizer, and mixed precision scaler initialized.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzoSdWk6wdDb",
        "outputId": "f80eff09-9b73-40a4-e13e-d29af1c2b36f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ BiSeNet loss, optimizer, and mixed precision scaler initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training BiseNet"
      ],
      "metadata": {
        "id": "j_5pi6UJwk98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "import os\n",
        "\n",
        "# ✅ Training config\n",
        "epochs = 50\n",
        "save_path = \"/content/drive/MyDrive/Semantic_Segmentation/bisenet_best_model.pth\"\n",
        "\n",
        "# ✅ Best validation tracking\n",
        "best_val_loss = float(\"inf\")\n",
        "\n",
        "print(\"🟢 Starting BiSeNet training...\")\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    loop = tqdm(train_loader, total=len(train_loader), desc=f\"Epoch [{epoch+1}/{epochs}]\", leave=False)\n",
        "\n",
        "    for images, targets in loop:\n",
        "        images = images.to(device)\n",
        "        targets = targets.squeeze(1).long().to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with autocast():\n",
        "            preds = model(images)  # returns (main, aux1, aux2)\n",
        "            loss = criterion(preds, targets)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        loop.set_postfix(loss=loss.item())\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "\n",
        "    # ✅ Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for val_imgs, val_masks in val_loader:\n",
        "            val_imgs = val_imgs.to(device)\n",
        "            val_masks = val_masks.squeeze(1).long().to(device)\n",
        "\n",
        "            with autocast():\n",
        "                 val_output = model(val_imgs)  # only main output\n",
        "                 loss = nn.CrossEntropyLoss(ignore_index=255)(val_output, val_masks)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "    # ✅ Save best model\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "        print(f\"💾 Best model saved at epoch {epoch+1} | Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    # ✅ Memory cleanup\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    mem_free = torch.cuda.mem_get_info()[0] / (1024 ** 3)\n",
        "\n",
        "    print(f\"✅ Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Free GPU: {mem_free:.2f} GB\")\n",
        "\n",
        "print(\"🏁 BiSeNet training complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWqxpjA7wjJc",
        "outputId": "782e11a8-f19d-4fdb-ad23-c206c8ba4365"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🟢 Starting BiSeNet training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Best model saved at epoch 1 | Val Loss: 0.6477\n",
            "✅ Epoch 1 | Train Loss: 1.4489 | Val Loss: 0.6477 | Free GPU: 21.35 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Best model saved at epoch 2 | Val Loss: 0.4815\n",
            "✅ Epoch 2 | Train Loss: 1.0314 | Val Loss: 0.4815 | Free GPU: 21.39 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Best model saved at epoch 3 | Val Loss: 0.4458\n",
            "✅ Epoch 3 | Train Loss: 0.8314 | Val Loss: 0.4458 | Free GPU: 21.35 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Best model saved at epoch 4 | Val Loss: 0.3915\n",
            "✅ Epoch 4 | Train Loss: 0.7300 | Val Loss: 0.3915 | Free GPU: 21.41 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Best model saved at epoch 5 | Val Loss: 0.3614\n",
            "✅ Epoch 5 | Train Loss: 0.6658 | Val Loss: 0.3614 | Free GPU: 21.33 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 6 | Train Loss: 0.6206 | Val Loss: 0.4091 | Free GPU: 21.31 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Best model saved at epoch 7 | Val Loss: 0.3555\n",
            "✅ Epoch 7 | Train Loss: 0.6041 | Val Loss: 0.3555 | Free GPU: 21.33 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Best model saved at epoch 8 | Val Loss: 0.3476\n",
            "✅ Epoch 8 | Train Loss: 0.5468 | Val Loss: 0.3476 | Free GPU: 21.28 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 9 | Train Loss: 0.5190 | Val Loss: 0.3619 | Free GPU: 21.32 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Best model saved at epoch 10 | Val Loss: 0.3081\n",
            "✅ Epoch 10 | Train Loss: 0.4831 | Val Loss: 0.3081 | Free GPU: 21.38 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Best model saved at epoch 11 | Val Loss: 0.3059\n",
            "✅ Epoch 11 | Train Loss: 0.4627 | Val Loss: 0.3059 | Free GPU: 21.40 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Best model saved at epoch 12 | Val Loss: 0.2970\n",
            "✅ Epoch 12 | Train Loss: 0.4383 | Val Loss: 0.2970 | Free GPU: 21.41 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Best model saved at epoch 13 | Val Loss: 0.2806\n",
            "✅ Epoch 13 | Train Loss: 0.4112 | Val Loss: 0.2806 | Free GPU: 21.36 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 14 | Train Loss: 0.3931 | Val Loss: 0.2823 | Free GPU: 21.33 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Best model saved at epoch 15 | Val Loss: 0.2771\n",
            "✅ Epoch 15 | Train Loss: 0.3702 | Val Loss: 0.2771 | Free GPU: 21.33 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 16 | Train Loss: 0.3628 | Val Loss: 0.3317 | Free GPU: 21.30 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Best model saved at epoch 17 | Val Loss: 0.2617\n",
            "✅ Epoch 17 | Train Loss: 0.3452 | Val Loss: 0.2617 | Free GPU: 21.35 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 18 | Train Loss: 0.3271 | Val Loss: 0.2642 | Free GPU: 21.32 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 19 | Train Loss: 0.3219 | Val Loss: 0.2966 | Free GPU: 21.33 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 20 | Train Loss: 0.3263 | Val Loss: 0.2726 | Free GPU: 21.41 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 21 | Train Loss: 0.2998 | Val Loss: 0.2822 | Free GPU: 21.41 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 22 | Train Loss: 0.2875 | Val Loss: 0.2669 | Free GPU: 21.32 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 23 | Train Loss: 0.2782 | Val Loss: 0.2711 | Free GPU: 21.32 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Best model saved at epoch 24 | Val Loss: 0.2576\n",
            "✅ Epoch 24 | Train Loss: 0.2669 | Val Loss: 0.2576 | Free GPU: 21.35 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 25 | Train Loss: 0.2644 | Val Loss: 0.2699 | Free GPU: 21.33 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Best model saved at epoch 26 | Val Loss: 0.2568\n",
            "✅ Epoch 26 | Train Loss: 0.2586 | Val Loss: 0.2568 | Free GPU: 21.37 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 27 | Train Loss: 0.2503 | Val Loss: 0.2653 | Free GPU: 21.31 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 28 | Train Loss: 0.2434 | Val Loss: 0.2620 | Free GPU: 21.40 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 29 | Train Loss: 0.2392 | Val Loss: 0.2664 | Free GPU: 21.24 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 30 | Train Loss: 0.2331 | Val Loss: 0.2840 | Free GPU: 21.27 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 31 | Train Loss: 0.2301 | Val Loss: 0.2724 | Free GPU: 21.33 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 32 | Train Loss: 0.2276 | Val Loss: 0.2683 | Free GPU: 21.29 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 33 | Train Loss: 0.2209 | Val Loss: 0.2612 | Free GPU: 21.31 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 34 | Train Loss: 0.2145 | Val Loss: 0.2708 | Free GPU: 21.39 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 35 | Train Loss: 0.2119 | Val Loss: 0.2690 | Free GPU: 21.42 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 36 | Train Loss: 0.2068 | Val Loss: 0.2710 | Free GPU: 21.28 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 37 | Train Loss: 0.2029 | Val Loss: 0.2792 | Free GPU: 21.37 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 38 | Train Loss: 0.1988 | Val Loss: 0.2832 | Free GPU: 21.44 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 39 | Train Loss: 0.1962 | Val Loss: 0.2811 | Free GPU: 21.42 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 40 | Train Loss: 0.1944 | Val Loss: 0.2797 | Free GPU: 21.31 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 41 | Train Loss: 0.1985 | Val Loss: 0.2758 | Free GPU: 21.31 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 42 | Train Loss: 0.2174 | Val Loss: 0.2832 | Free GPU: 21.23 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 43 | Train Loss: 0.1937 | Val Loss: 0.2808 | Free GPU: 21.27 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 44 | Train Loss: 0.1855 | Val Loss: 0.2854 | Free GPU: 21.33 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 45 | Train Loss: 0.1821 | Val Loss: 0.2812 | Free GPU: 21.35 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 46 | Train Loss: 0.1783 | Val Loss: 0.2824 | Free GPU: 21.34 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 47 | Train Loss: 0.1789 | Val Loss: 0.2870 | Free GPU: 21.29 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 48 | Train Loss: 0.1771 | Val Loss: 0.2930 | Free GPU: 21.34 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 49 | Train Loss: 0.1726 | Val Loss: 0.2916 | Free GPU: 21.36 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 50 | Train Loss: 0.1706 | Val Loss: 0.2909 | Free GPU: 21.40 GB\n",
            "🏁 BiSeNet training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/MLDL2024_project1\")\n"
      ],
      "metadata": {
        "id": "1ClpMCKxK-xd"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/MLDL2024_project1/models/bisenet\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "riN9unK1Ly7K",
        "outputId": "7579ac51-96e4-4177-c0dc-702120a99ae2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "build_bisenet.py  build_contextpath.py\t__pycache__\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/MLDL2024_project1\")\n",
        "\n",
        "from models.bisenet.build_bisenet import BiSeNet\n"
      ],
      "metadata": {
        "id": "ojto-gHvMohi"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Evaluation"
      ],
      "metadata": {
        "id": "UbU6rtxeUnXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Path to your saved model\n",
        "model_path = \"/content/drive/MyDrive/Semantic_Segmentation/bisenet_best_model.pth\"\n",
        "\n",
        "# Initialize the model with the correct number of classes and backbone\n",
        "model = BiSeNet(num_classes=19, context_path=\"resnet18\")\n",
        "model.load_state_dict(torch.load(model_path, map_location='cuda'))\n",
        "model.eval().to('cuda')\n",
        "\n",
        "print(\"✅ BiSeNet model loaded and ready for evaluation.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baGJ0o_QNPE2",
        "outputId": "b42782c3-6b33-4410-923b-d18c6938be8e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ BiSeNet model loaded and ready for evaluation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "def compute_miou(preds, labels, num_classes=19):\n",
        "    ious = []\n",
        "    preds = preds.view(-1).cpu().numpy()\n",
        "    labels = labels.view(-1).cpu().numpy()\n",
        "    for cls in range(num_classes):\n",
        "        pred_inds = preds == cls\n",
        "        target_inds = labels == cls\n",
        "        intersection = (pred_inds & target_inds).sum()\n",
        "        union = (pred_inds | target_inds).sum()\n",
        "        if union == 0:\n",
        "            ious.append(float('nan'))  # Skip class\n",
        "        else:\n",
        "            ious.append(intersection / union)\n",
        "    return np.nanmean(ious)\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "ious = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, masks in tqdm(val_loader, desc=\"🔍 Evaluating mIoU\"):\n",
        "        imgs = imgs.to('cuda')\n",
        "        masks = masks.squeeze(1).long().to('cuda')\n",
        "\n",
        "        outputs = model(imgs)\n",
        "        if isinstance(outputs, tuple):  # Only use main output if multiple\n",
        "            outputs = outputs[0]\n",
        "\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        iou = compute_miou(preds, masks, num_classes=19)\n",
        "        ious.append(iou)\n",
        "\n",
        "mean_iou = np.nanmean(ious)\n",
        "print(f\"📊 Final mIoU: {mean_iou:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSfAc3vbNwhg",
        "outputId": "ba2b1dde-9bc3-42d1-dad5-3232740f2b5d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "🔍 Evaluating mIoU: 100%|██████████| 250/250 [00:30<00:00,  8.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 Final mIoU: 0.4211\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "# ✅ Ensure the model is in evaluation mode and on CUDA\n",
        "model.eval()\n",
        "model.to('cuda')\n",
        "\n",
        "# ✅ input: batch size = 1, 3 channels, 512 height, 1024 width\n",
        "input_tensor = torch.randn(1, 3, 512, 1024).to('cuda')\n",
        "\n",
        "# ✅ Warm-up to stabilize performance\n",
        "for _ in range(10):\n",
        "    with torch.no_grad():\n",
        "        _ = model(input_tensor)\n",
        "\n",
        "# ✅ Measure latency\n",
        "latencies = []\n",
        "iterations = 100\n",
        "\n",
        "for _ in range(iterations):\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        _ = model(input_tensor)\n",
        "    torch.cuda.synchronize()\n",
        "    end_time = time.time()\n",
        "    latencies.append((end_time - start_time) * 1000)  # convert to ms\n",
        "\n",
        "# ✅ Compute results\n",
        "mean_latency = np.mean(latencies)\n",
        "std_latency = np.std(latencies)\n",
        "fps = 1000 / mean_latency\n",
        "\n",
        "print(f\"⏱️ Latency: {mean_latency:.2f} ± {std_latency:.2f} ms/image\")\n",
        "print(f\"🚀 FPS: {fps:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QcH6znGnPU5T",
        "outputId": "7d4a1b2c-03b8-47cd-ce98-df8faff4cc4e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⏱️ Latency: 6.96 ± 0.84 ms/image\n",
            "🚀 FPS: 143.72\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q fvcore\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_3-10ePR9Wx",
        "outputId": "c4980456-84f5-4d4a-d5ad-2f0afc9ae3d6"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from fvcore.nn import FlopCountAnalysis, parameter_count_table\n",
        "from models.bisenet.build_bisenet import BiSeNet\n",
        "\n",
        "# ✅ Reinitialize the model (if needed)\n",
        "model = BiSeNet(num_classes=19, context_path='resnet18').to('cuda')\n",
        "model.eval()\n",
        "\n",
        "# ✅ input matching Cityscapes size\n",
        "dummy_input = torch.randn(1, 3, 512, 1024).to('cuda')\n",
        "\n",
        "# ✅ Compute FLOPs and parameters\n",
        "flops = FlopCountAnalysis(model, dummy_input)\n",
        "params = parameter_count_table(model)\n",
        "\n",
        "print(f\"🔢 Total FLOPs: {flops.total() / 1e9:.2f} GFLOPs\")\n",
        "print(params)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-k6Uzk2TCID",
        "outputId": "e4c9303a-e2db-45b9-a250-f5cab9048df7"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::max_pool2d encountered 1 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::add_ encountered 8 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::mean encountered 2 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::sigmoid encountered 3 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::mul encountered 4 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::add encountered 1 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
            "context_path.features.avgpool, context_path.features.fc, supervision1, supervision2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔢 Total FLOPs: 25.78 GFLOPs\n",
            "| name                                        | #elements or shape   |\n",
            "|:--------------------------------------------|:---------------------|\n",
            "| model                                       | 12.6M                |\n",
            "|  saptial_path                               |  0.4M                |\n",
            "|   saptial_path.convblock1                   |   1.9K               |\n",
            "|    saptial_path.convblock1.conv1            |    1.7K              |\n",
            "|    saptial_path.convblock1.bn               |    0.1K              |\n",
            "|   saptial_path.convblock2                   |   74.0K              |\n",
            "|    saptial_path.convblock2.conv1            |    73.7K             |\n",
            "|    saptial_path.convblock2.bn               |    0.3K              |\n",
            "|   saptial_path.convblock3                   |   0.3M               |\n",
            "|    saptial_path.convblock3.conv1            |    0.3M              |\n",
            "|    saptial_path.convblock3.bn               |    0.5K              |\n",
            "|  context_path                               |  11.7M               |\n",
            "|   context_path.features                     |   11.7M              |\n",
            "|    context_path.features.conv1              |    9.4K              |\n",
            "|    context_path.features.bn1                |    0.1K              |\n",
            "|    context_path.features.layer1             |    0.1M              |\n",
            "|    context_path.features.layer2             |    0.5M              |\n",
            "|    context_path.features.layer3             |    2.1M              |\n",
            "|    context_path.features.layer4             |    8.4M              |\n",
            "|    context_path.features.fc                 |    0.5M              |\n",
            "|  attention_refinement_module1               |  66.3K               |\n",
            "|   attention_refinement_module1.conv         |   65.8K              |\n",
            "|    attention_refinement_module1.conv.weight |    (256, 256, 1, 1)  |\n",
            "|    attention_refinement_module1.conv.bias   |    (256,)            |\n",
            "|   attention_refinement_module1.bn           |   0.5K               |\n",
            "|    attention_refinement_module1.bn.weight   |    (256,)            |\n",
            "|    attention_refinement_module1.bn.bias     |    (256,)            |\n",
            "|  attention_refinement_module2               |  0.3M                |\n",
            "|   attention_refinement_module2.conv         |   0.3M               |\n",
            "|    attention_refinement_module2.conv.weight |    (512, 512, 1, 1)  |\n",
            "|    attention_refinement_module2.conv.bias   |    (512,)            |\n",
            "|   attention_refinement_module2.bn           |   1.0K               |\n",
            "|    attention_refinement_module2.bn.weight   |    (512,)            |\n",
            "|    attention_refinement_module2.bn.bias     |    (512,)            |\n",
            "|  supervision1                               |  4.9K                |\n",
            "|   supervision1.weight                       |   (19, 256, 1, 1)    |\n",
            "|   supervision1.bias                         |   (19,)              |\n",
            "|  supervision2                               |  9.7K                |\n",
            "|   supervision2.weight                       |   (19, 512, 1, 1)    |\n",
            "|   supervision2.bias                         |   (19,)              |\n",
            "|  feature_fusion_module                      |  0.2M                |\n",
            "|   feature_fusion_module.convblock           |   0.2M               |\n",
            "|    feature_fusion_module.convblock.conv1    |    0.2M              |\n",
            "|    feature_fusion_module.convblock.bn       |    38                |\n",
            "|   feature_fusion_module.conv1               |   0.4K               |\n",
            "|    feature_fusion_module.conv1.weight       |    (19, 19, 1, 1)    |\n",
            "|    feature_fusion_module.conv1.bias         |    (19,)             |\n",
            "|   feature_fusion_module.conv2               |   0.4K               |\n",
            "|    feature_fusion_module.conv2.weight       |    (19, 19, 1, 1)    |\n",
            "|    feature_fusion_module.conv2.bias         |    (19,)             |\n",
            "|  conv                                       |  0.4K                |\n",
            "|   conv.weight                               |   (19, 19, 1, 1)     |\n",
            "|   conv.bias                                 |   (19,)              |\n"
          ]
        }
      ]
    }
  ]
}