{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHmYCDjc3Q4b",
        "outputId": "8e2e1060-b527-4761-fff0-c48de2de4c7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Define paths\n",
        "gta5_zip = \"/content/drive/MyDrive/Semantic_Segmentation/GTA5.zip\"\n",
        "cityscapes_zip = \"/content/drive/MyDrive/Semantic_Segmentation/Cityscapes.zip\"\n",
        "\n",
        "os.makedirs(\"/content/datasets/GTA5\", exist_ok=True)\n",
        "os.makedirs(\"/content/datasets/Cityscapes\", exist_ok=True)\n",
        "\n",
        "# Unzip GTA5\n",
        "with zipfile.ZipFile(gta5_zip, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"/content/datasets/GTA5\")\n",
        "\n",
        "# Unzip Cityscapes\n",
        "with zipfile.ZipFile(cityscapes_zip, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"/content/datasets/Cityscapes\")\n",
        "\n",
        "print(\"Both datasets extracted.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwyQUFae3eMb",
        "outputId": "469ffa78-3785-432e-a39c-438dfb959a93"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Both datasets extracted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Fix GTA5\n",
        "if os.path.exists(\"/content/datasets/GTA5/GTA5\"):\n",
        "    shutil.move(\"/content/datasets/GTA5/GTA5/images\", \"/content/datasets/GTA5/images\")\n",
        "    shutil.move(\"/content/datasets/GTA5/GTA5/labels\", \"/content/datasets/GTA5/labels\")\n",
        "    shutil.rmtree(\"/content/datasets/GTA5/GTA5\")\n",
        "    print(\"Fixed GTA5 folder structure\")\n",
        "\n",
        "# Fix Cityscapes\n",
        "nested_city = \"/content/datasets/Cityscapes/Cityscapes/Cityspaces\"\n",
        "if os.path.exists(nested_city):\n",
        "    shutil.move(os.path.join(nested_city, \"images\"), \"/content/datasets/Cityscapes/leftImg8bit\")\n",
        "    shutil.move(os.path.join(nested_city, \"gtFine\"), \"/content/datasets/Cityscapes/gtFine\")\n",
        "    shutil.rmtree(\"/content/datasets/Cityscapes/Cityscapes\")\n",
        "    print(\"Fixed Cityscapes folder structure\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8Zd6x6r3hWr",
        "outputId": "6d402ac2-8e9f-48a1-c892-2f032405cc1b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fixed GTA5 folder structure\n",
            "Fixed Cityscapes folder structure\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clone the BiSeNet Repo & Import the Model"
      ],
      "metadata": {
        "id": "utDDDMnJ3mGX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the repo (your custom version)\n",
        "!git clone https://github.com/Gabrysse/MLDL2024_project1.git\n",
        "\n",
        "# Add to Python path for import\n",
        "import sys\n",
        "sys.path.append(\"/content/MLDL2024_project1\")\n",
        "\n",
        "# Import BiSeNet\n",
        "from models.bisenet.build_bisenet import BiSeNet\n",
        "import torch\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(\"âœ… BiSeNet repo cloned and model class imported.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGyQ2mLK3jHl",
        "outputId": "b62c1fb9-83bc-4811-b62f-7c22587be630"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MLDL2024_project1'...\n",
            "remote: Enumerating objects: 34, done.\u001b[K\n",
            "remote: Counting objects: 100% (21/21), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 34 (delta 9), reused 3 (delta 3), pack-reused 13 (from 1)\u001b[K\n",
            "Receiving objects: 100% (34/34), 11.29 KiB | 11.29 MiB/s, done.\n",
            "Resolving deltas: 100% (9/9), done.\n",
            "âœ… BiSeNet repo cloned and model class imported.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load FDA-Trained Weights into BiSeNet"
      ],
      "metadata": {
        "id": "pRmBuwrC3pGt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from models.bisenet.build_bisenet import BiSeNet\n",
        "import torch\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Build model (uses ResNet-18 by default as per repo structure)\n",
        "model = BiSeNet(19, 'resnet18')\n",
        "model = model.to(device)\n",
        "\n",
        "# Load FDA-trained weights\n",
        "checkpoint_path = \"/content/drive/MyDrive/Semantic_Segmentation/bisenet_gta5_fda_final.pth\"\n",
        "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "model.load_state_dict(checkpoint)\n",
        "\n",
        "# Set to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "print(\"BiSeNet loaded with FDA-trained weights.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJpQBrkb3pgN",
        "outputId": "ebcfbdef-f142-4408-cff6-08740d0a752f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44.7M/44.7M [00:00<00:00, 225MB/s]\n",
            "Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 171M/171M [00:01<00:00, 150MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BiSeNet loaded with FDA-trained weights.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate Pseudo-Labels using FDA-Trained BiSeNet"
      ],
      "metadata": {
        "id": "EmUh9p7U3sfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "# Define paths\n",
        "image_dir = \"/content/datasets/Cityscapes/leftImg8bit/train\"\n",
        "pseudo_label_dir = \"/content/pseudo_labels_dice\"\n",
        "os.makedirs(pseudo_label_dir, exist_ok=True)\n",
        "\n",
        "# Define image preprocessing\n",
        "transform_img = transforms.Compose([\n",
        "    transforms.Resize((720, 1280)),  # Resize to BiSeNet input\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Gather all image paths\n",
        "all_image_paths = []\n",
        "for city in os.listdir(image_dir):\n",
        "    city_path = os.path.join(image_dir, city)\n",
        "    for fname in os.listdir(city_path):\n",
        "        if fname.endswith(\"_leftImg8bit.png\"):\n",
        "            all_image_paths.append(os.path.join(city_path, fname))\n",
        "\n",
        "print(f\"ðŸ” Found {len(all_image_paths)} images in Cityscapes/train\")\n",
        "\n",
        "# Generate pseudo-labels\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for img_path in tqdm(all_image_paths, desc=\"âš™ï¸ Generating Pseudo-labels\"):\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        input_tensor = transform_img(img).unsqueeze(0).to(device)  # (1, 3, H, W)\n",
        "\n",
        "        output = model(input_tensor)[0]  # output shape: (1, 19, H, W)\n",
        "        pred = torch.argmax(output.squeeze(), dim=0).cpu().numpy().astype(np.uint8)\n",
        "\n",
        "        # Save predicted mask\n",
        "        out_name = os.path.basename(img_path).replace(\"_leftImg8bit.png\", \"_pseudo_label.png\")\n",
        "        out_path = os.path.join(pseudo_label_dir, out_name)\n",
        "        Image.fromarray(pred).save(out_path)\n",
        "\n",
        "print(f\"âœ… All pseudo-labels saved to: {pseudo_label_dir}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vU1eDaFq3s7N",
        "outputId": "50c38d50-8fe0-4d72-df67-c68f8eda413f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ” Found 1572 images in Cityscapes/train\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "âš™ï¸ Generating Pseudo-labels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1572/1572 [04:28<00:00,  5.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… All pseudo-labels saved to: /content/pseudo_labels_dice\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rebuild Dataset Classes for Hybrid Training"
      ],
      "metadata": {
        "id": "LaBJj2xG6VlV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import os\n",
        "import torch\n",
        "import random\n",
        "\n",
        "# Transforms\n",
        "transform_image = transforms.Compose([\n",
        "    transforms.Resize((720, 1280)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "transform_label = transforms.Compose([\n",
        "    transforms.Resize((720, 1280), interpolation=Image.NEAREST),\n",
        "    transforms.PILToTensor()\n",
        "])\n",
        "\n",
        "# GTA5 Dataset\n",
        "class GTA5Dataset(Dataset):\n",
        "    def __init__(self, root, transform_img, transform_lbl):\n",
        "        self.img_dir = os.path.join(root, \"images\")\n",
        "        self.lbl_dir = os.path.join(root, \"labels\")\n",
        "        self.imgs = sorted(os.listdir(self.img_dir))\n",
        "        self.transform_img = transform_img\n",
        "        self.transform_lbl = transform_lbl\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, self.imgs[idx])\n",
        "        lbl_path = os.path.join(self.lbl_dir, self.imgs[idx].replace(\"_leftImg8bit.png\", \"_gtFine_labelIds.png\"))\n",
        "\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        lbl = Image.open(lbl_path)\n",
        "\n",
        "        if self.transform_img:\n",
        "            img = self.transform_img(img)\n",
        "        if self.transform_lbl:\n",
        "            lbl = self.transform_lbl(lbl).squeeze().long()\n",
        "\n",
        "        return img, lbl\n",
        "\n",
        "# Cityscapes Val Dataset\n",
        "class CityscapesValDataset(Dataset):\n",
        "    def __init__(self, root, transform_img, transform_lbl):\n",
        "        self.img_dir = os.path.join(root, \"leftImg8bit\", \"val\")\n",
        "        self.lbl_dir = os.path.join(root, \"gtFine\", \"val\")\n",
        "        self.imgs = []\n",
        "        for city in os.listdir(self.img_dir):\n",
        "            for fname in os.listdir(os.path.join(self.img_dir, city)):\n",
        "                if fname.endswith(\"_leftImg8bit.png\"):\n",
        "                    self.imgs.append(os.path.join(city, fname))\n",
        "        self.transform_img = transform_img\n",
        "        self.transform_lbl = transform_lbl\n",
        "        self.root = root\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_rel = self.imgs[idx]\n",
        "        city, fname = img_rel.split('/')\n",
        "        lbl_name = fname.replace(\"_leftImg8bit.png\", \"_gtFine_labelIds.png\")\n",
        "\n",
        "        img = Image.open(os.path.join(self.img_dir, city, fname)).convert(\"RGB\")\n",
        "        lbl = Image.open(os.path.join(self.lbl_dir, city, lbl_name))\n",
        "\n",
        "        if self.transform_img:\n",
        "            img = self.transform_img(img)\n",
        "        if self.transform_lbl:\n",
        "            lbl = self.transform_lbl(lbl).squeeze().long()\n",
        "\n",
        "        return img, lbl\n",
        "\n",
        "# DACS Dataset\n",
        "class DACSDataset(Dataset):\n",
        "    def __init__(self, gta5_dataset, cityscapes_root, pseudo_root, transform_img, transform_lbl, ignore_index=255):\n",
        "        self.gta5_dataset = gta5_dataset\n",
        "        self.transform_img = transform_img\n",
        "        self.transform_lbl = transform_lbl\n",
        "        self.ignore_index = ignore_index\n",
        "\n",
        "        self.city_imgs = []\n",
        "        self.pseudo_labels = []\n",
        "        for city in os.listdir(os.path.join(cityscapes_root, \"leftImg8bit\", \"train\")):\n",
        "            city_path = os.path.join(cityscapes_root, \"leftImg8bit\", \"train\", city)\n",
        "            for fname in os.listdir(city_path):\n",
        "                if fname.endswith(\"_leftImg8bit.png\"):\n",
        "                    self.city_imgs.append(os.path.join(city_path, fname))\n",
        "                    pseudo = os.path.join(pseudo_root, fname.replace(\"_leftImg8bit.png\", \"_pseudo_label.png\"))\n",
        "                    self.pseudo_labels.append(pseudo)\n",
        "\n",
        "    def __len__(self):\n",
        "        return min(len(self.gta5_dataset), len(self.city_imgs))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src_img, src_lbl = self.gta5_dataset[idx]\n",
        "        tgt_img = Image.open(self.city_imgs[idx]).convert(\"RGB\")\n",
        "        tgt_lbl = Image.open(self.pseudo_labels[idx])\n",
        "\n",
        "        tgt_img = self.transform_img(tgt_img)\n",
        "        tgt_lbl = self.transform_lbl(tgt_lbl).squeeze().long()\n",
        "        tgt_lbl = torch.clamp(tgt_lbl, 0, 18)\n",
        "\n",
        "        # ClassMix\n",
        "        classes = torch.unique(src_lbl)\n",
        "        classes = classes[classes != self.ignore_index]\n",
        "        selected = classes[torch.randperm(len(classes))[:len(classes)//2]] if len(classes) > 0 else torch.tensor([], dtype=torch.long)\n",
        "        mask = torch.zeros_like(src_lbl, dtype=torch.bool)\n",
        "        for c in selected:\n",
        "            mask[src_lbl == c] = True\n",
        "        mask = mask.unsqueeze(0)\n",
        "\n",
        "        # Handle ignore index during mixing\n",
        "        mixed_lbl = torch.where(mask.squeeze(0), src_lbl, tgt_lbl)\n",
        "        # Ensure ignore index from source is preserved where mask is true\n",
        "        mixed_lbl = torch.where((mask.squeeze(0) & (src_lbl == self.ignore_index)), self.ignore_index, mixed_lbl)\n",
        "\n",
        "\n",
        "        mixed_img = tgt_img * (~mask) + src_img * mask.float()\n",
        "\n",
        "\n",
        "        return mixed_img, mixed_lbl"
      ],
      "metadata": {
        "id": "kNV1o0xJ6fNf"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Define Transforms & Initialize DataLoaders"
      ],
      "metadata": {
        "id": "BuLFBmpd7GYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Paths\n",
        "gta5_root = \"/content/datasets/GTA5\"\n",
        "cityscapes_root = \"/content/datasets/Cityscapes\"\n",
        "pseudo_label_dir = \"/content/pseudo_labels_dice\"\n",
        "\n",
        "# Initialize datasets\n",
        "gta5_dataset = GTA5Dataset(gta5_root, transform_image, transform_label)\n",
        "val_dataset  = CityscapesValDataset(cityscapes_root, transform_image, transform_label)\n",
        "dacs_dataset = DACSDataset(gta5_dataset, cityscapes_root, pseudo_label_dir, transform_image, transform_label)\n",
        "\n",
        "# DataLoaders\n",
        "gta5_loader = DataLoader(gta5_dataset, batch_size=2, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)\n",
        "val_loader  = DataLoader(val_dataset, batch_size=2, shuffle=False, num_workers=4)\n",
        "dacs_loader = DataLoader(dacs_dataset, batch_size=2, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)\n",
        "\n",
        "# âœ… Sanity check\n",
        "batch_imgs, batch_lbls = next(iter(dacs_loader))\n",
        "print(\"DACS âœ…\", batch_imgs.shape, batch_lbls.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4iK7dlW7G8C",
        "outputId": "486630cb-2e92-4b5c-c47c-33a610861ac2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DACS âœ… torch.Size([2, 3, 720, 1280]) torch.Size([2, 720, 1280])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DataLoader Setup"
      ],
      "metadata": {
        "id": "0zq9p10n_HjF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# âœ… Dataset paths\n",
        "gta5_root = \"/content/datasets/GTA5\"\n",
        "cityscapes_root = \"/content/datasets/Cityscapes\"\n",
        "pseudo_label_dir = \"/content/pseudo_labels_dice\"  # updated pseudo-label path\n",
        "\n",
        "# âœ… Initialize datasets\n",
        "gta5_dataset = GTA5Dataset(gta5_root, transform_image, transform_label)\n",
        "val_dataset  = CityscapesValDataset(cityscapes_root, transform_image, transform_label)\n",
        "dacs_dataset = DACSDataset(gta5_dataset, cityscapes_root, pseudo_label_dir, transform_image, transform_label)\n",
        "\n",
        "# âœ… Initialize DataLoaders\n",
        "gta5_loader = DataLoader(gta5_dataset, batch_size=2, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)\n",
        "val_loader  = DataLoader(val_dataset, batch_size=2, shuffle=False, num_workers=4)\n",
        "dacs_loader = DataLoader(dacs_dataset, batch_size=2, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)\n",
        "\n",
        "# âœ… Sanity check\n",
        "sample_imgs, sample_lbls = next(iter(dacs_loader))\n",
        "print(\"DACS DataLoader âœ…\", sample_imgs.shape, sample_lbls.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ys8c9-zX_mba",
        "outputId": "ec66f5e9-a1d6-4c3e-d0e2-bd370b0b312c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DACS DataLoader âœ… torch.Size([2, 3, 720, 1280]) torch.Size([2, 720, 1280])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Hybrid Loss (CrossEntropy + Dice)"
      ],
      "metadata": {
        "id": "6VuC3AE5BKQS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "HybridSegmentationLoss"
      ],
      "metadata": {
        "id": "U2pE6hDeBOXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class HybridSegmentationLoss(nn.Module):\n",
        "    def __init__(self, weight_dice=1.0, ignore_index=255):\n",
        "        super(HybridSegmentationLoss, self).__init__()\n",
        "        self.ce_loss = nn.CrossEntropyLoss(ignore_index=ignore_index)\n",
        "        self.weight_dice = weight_dice\n",
        "        self.ignore_index = ignore_index\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        # logits: (B, C, H, W)\n",
        "        # targets: (B, H, W)\n",
        "\n",
        "        ce = self.ce_loss(logits, targets)\n",
        "\n",
        "        if self.weight_dice == 0:\n",
        "            return ce\n",
        "\n",
        "        # Convert targets to one-hot format for dice\n",
        "        num_classes = logits.shape[1]\n",
        "        # Mask out invalid pixels\n",
        "        valid_mask = (targets != self.ignore_index)\n",
        "\n",
        "        # Clamp invalid class labels\n",
        "        targets_clamped = targets.clone()\n",
        "        targets_clamped[~valid_mask] = 0  # temp set to 0 just for one_hot\n",
        "        targets_one_hot = F.one_hot(targets.clamp(0, num_classes - 1), num_classes=num_classes)  # (B, H, W, C)\n",
        "        targets_one_hot = targets_one_hot.permute(0, 3, 1, 2).float()  # (B, C, H, W)\n",
        "\n",
        "        probs = torch.softmax(logits, dim=1)  # (B, C, H, W)\n",
        "\n",
        "        # Mask ignore pixels\n",
        "        valid_mask = valid_mask.unsqueeze(1)  # (B, 1, H, W)\n",
        "        probs = probs * valid_mask\n",
        "        targets_one_hot = targets_one_hot * valid_mask\n",
        "\n",
        "        # Dice calculation\n",
        "        eps = 1e-7\n",
        "        intersection = (probs * targets_one_hot).sum(dim=(2, 3))\n",
        "        union = probs.sum(dim=(2, 3)) + targets_one_hot.sum(dim=(2, 3))\n",
        "        dice = 1 - (2 * intersection + eps) / (union + eps)\n",
        "        dice_loss = dice.mean()\n",
        "\n",
        "        return ce + self.weight_dice * dice_loss\n"
      ],
      "metadata": {
        "id": "uPUCRVlvBKwJ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from tqdm import tqdm\n",
        "\n",
        "# === CONFIG ===\n",
        "num_epochs = 10\n",
        "save_dir = \"/content/drive/MyDrive/Semantic_Segmentation/extension_checkpoints\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "best_model_path = os.path.join(save_dir, \"best_model_dacs_dice.pth\")\n",
        "checkpoint_path = os.path.join(save_dir, \"last_checkpoint.pth\")\n",
        "ignore_index = 255 # Define ignore index here\n",
        "\n",
        "# === MODEL ===\n",
        "model = BiSeNet(num_classes=19, context_path='resnet18').to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
        "scaler = GradScaler()\n",
        "criterion = HybridSegmentationLoss(weight_dice=1.0, ignore_index=ignore_index).to(device)\n",
        "val_criterion = torch.nn.CrossEntropyLoss(ignore_index=ignore_index) # Set ignore_index for validation loss\n",
        "\n",
        "# === RESUME CHECKPOINT IF EXISTS ===\n",
        "start_epoch = 0\n",
        "best_val_loss = float(\"inf\")\n",
        "if os.path.exists(checkpoint_path):\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    scaler.load_state_dict(checkpoint['scaler'])\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "    best_val_loss = checkpoint['best_val_loss']\n",
        "    print(f\"Resumed training from epoch {start_epoch}\")\n",
        "\n",
        "# === TRAINING LOOP ===\n",
        "for epoch in range(start_epoch, num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    loop = tqdm(dacs_loader, desc=f\"Epoch {epoch+1}\", leave=False)\n",
        "\n",
        "    for imgs, labels in loop:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        with autocast():\n",
        "            logits = model(imgs)[0]\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        loop.set_postfix(train_loss=loss.item())\n",
        "\n",
        "    avg_train_loss = total_loss / len(dacs_loader)\n",
        "\n",
        "    # === VALIDATION ===\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for v_imgs, v_labels in val_loader:\n",
        "            v_imgs, v_labels = v_imgs.to(device), v_labels.to(device)\n",
        "            v_logits = model(v_imgs)[0]\n",
        "            v_loss = val_criterion(v_logits, v_labels)\n",
        "            val_loss += v_loss.item()\n",
        "    val_loss /= len(val_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    # === SAVE BEST MODEL ===\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(f\"Saved Best Model at Epoch {epoch+1}\")\n",
        "\n",
        "    # === SAVE CHECKPOINT ===\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "        'scaler': scaler.state_dict(),\n",
        "        'best_val_loss': best_val_loss\n",
        "    }, checkpoint_path)\n",
        "\n",
        "print(\"Training Complete\")"
      ],
      "metadata": {
        "id": "w1PuifpNHpzl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}