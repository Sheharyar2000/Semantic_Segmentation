{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxYDypgdqnSs",
        "outputId": "f2b30c7a-b344-41ca-c6ac-218af4b95afe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avmznu6JrDI_",
        "outputId": "539cde24-0b6c-4aa8-c9dd-85191fc825e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Both datasets extracted.\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Define paths\n",
        "gta5_zip = \"/content/drive/MyDrive/Semantic_Segmentation/GTA5.zip\"\n",
        "cityscapes_zip = \"/content/drive/MyDrive/Semantic_Segmentation/Cityscapes.zip\"\n",
        "\n",
        "os.makedirs(\"/content/datasets/GTA5\", exist_ok=True)\n",
        "os.makedirs(\"/content/datasets/Cityscapes\", exist_ok=True)\n",
        "\n",
        "# Unzip GTA5\n",
        "with zipfile.ZipFile(gta5_zip, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"/content/datasets/GTA5\")\n",
        "\n",
        "# Unzip Cityscapes\n",
        "with zipfile.ZipFile(cityscapes_zip, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"/content/datasets/Cityscapes\")\n",
        "\n",
        "print(\"Both datasets extracted.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnS1mCHGseWP",
        "outputId": "5edeff5d-7721-44c3-9df0-8f0df8d38b4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fixed GTA5 folder structure\n",
            "Fixed Cityscapes folder structure\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Fix GTA5\n",
        "if os.path.exists(\"/content/datasets/GTA5/GTA5\"):\n",
        "    shutil.move(\"/content/datasets/GTA5/GTA5/images\", \"/content/datasets/GTA5/images\")\n",
        "    shutil.move(\"/content/datasets/GTA5/GTA5/labels\", \"/content/datasets/GTA5/labels\")\n",
        "    shutil.rmtree(\"/content/datasets/GTA5/GTA5\")\n",
        "    print(\"Fixed GTA5 folder structure\")\n",
        "\n",
        "# Fix Cityscapes\n",
        "nested_city = \"/content/datasets/Cityscapes/Cityscapes/Cityspaces\"\n",
        "if os.path.exists(nested_city):\n",
        "    shutil.move(os.path.join(nested_city, \"images\"), \"/content/datasets/Cityscapes/leftImg8bit\")\n",
        "    shutil.move(os.path.join(nested_city, \"gtFine\"), \"/content/datasets/Cityscapes/gtFine\")\n",
        "    shutil.rmtree(\"/content/datasets/Cityscapes/Cityscapes\")\n",
        "    print(\"Fixed Cityscapes folder structure\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpYo1zoMsgsX",
        "outputId": "b30c66e6-dc32-475b-c108-11e9e8a0187a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MLDL2024_project1'...\n",
            "remote: Enumerating objects: 34, done.\u001b[K\n",
            "remote: Counting objects: 100% (21/21), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 34 (delta 9), reused 3 (delta 3), pack-reused 13 (from 1)\u001b[K\n",
            "Receiving objects: 100% (34/34), 11.29 KiB | 5.64 MiB/s, done.\n",
            "Resolving deltas: 100% (9/9), done.\n",
            "BiSeNet repo cloned and model class imported.\n"
          ]
        }
      ],
      "source": [
        "# Clone the repo (your custom version)\n",
        "!git clone https://github.com/Gabrysse/MLDL2024_project1.git\n",
        "\n",
        "# Add to Python path for import\n",
        "import sys\n",
        "sys.path.append(\"/content/MLDL2024_project1\")\n",
        "\n",
        "# Import BiSeNet\n",
        "from models.bisenet.build_bisenet import BiSeNet\n",
        "import torch\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(\"BiSeNet repo cloned and model class imported.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjT41WEPsmaQ",
        "outputId": "9c5385a5-0eca-480c-e90c-4742a6278283"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 194MB/s]\n",
            "Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n",
            "100%|██████████| 171M/171M [00:01<00:00, 170MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BiSeNet loaded with FDA-trained weights.\n"
          ]
        }
      ],
      "source": [
        "from models.bisenet.build_bisenet import BiSeNet\n",
        "import torch\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Build model (uses ResNet-18 by default as per repo structure)\n",
        "model = BiSeNet(19, 'resnet18')\n",
        "model = model.to(device)\n",
        "\n",
        "# Load FDA-trained weights\n",
        "checkpoint_path = \"/content/drive/MyDrive/Semantic_Segmentation/bisenet_gta5_fda_final.pth\"\n",
        "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "model.load_state_dict(checkpoint)\n",
        "\n",
        "# Set to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "print(\"BiSeNet loaded with FDA-trained weights.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tueOTEiswcp",
        "outputId": "5fcfd117-036a-4390-92bd-c6c31ca0d5d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1572 images in Cityscapes/train\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Pseudo-labels: 100%|██████████| 1572/1572 [04:43<00:00,  5.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All pseudo-labels saved to: /content/pseudo_labels_dice\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "# Define paths\n",
        "image_dir = \"/content/datasets/Cityscapes/leftImg8bit/train\"\n",
        "pseudo_label_dir = \"/content/pseudo_labels_dice\"\n",
        "os.makedirs(pseudo_label_dir, exist_ok=True)\n",
        "\n",
        "# Define image preprocessing\n",
        "transform_img = transforms.Compose([\n",
        "    transforms.Resize((720, 1280)),  # Resize to BiSeNet input\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Gather all image paths\n",
        "all_image_paths = []\n",
        "for city in os.listdir(image_dir):\n",
        "    city_path = os.path.join(image_dir, city)\n",
        "    for fname in os.listdir(city_path):\n",
        "        if fname.endswith(\"_leftImg8bit.png\"):\n",
        "            all_image_paths.append(os.path.join(city_path, fname))\n",
        "\n",
        "print(f\"Found {len(all_image_paths)} images in Cityscapes/train\")\n",
        "\n",
        "# Generate pseudo-labels\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for img_path in tqdm(all_image_paths, desc=\"Generating Pseudo-labels\"):\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        input_tensor = transform_img(img).unsqueeze(0).to(device)  # (1, 3, H, W)\n",
        "\n",
        "        output = model(input_tensor)[0]  # output shape: (1, 19, H, W)\n",
        "        pred = torch.argmax(output.squeeze(), dim=0).cpu().numpy().astype(np.uint8)\n",
        "\n",
        "        # Save predicted mask\n",
        "        out_name = os.path.basename(img_path).replace(\"_leftImg8bit.png\", \"_pseudo_label.png\")\n",
        "        out_path = os.path.join(pseudo_label_dir, out_name)\n",
        "        Image.fromarray(pred).save(out_path)\n",
        "\n",
        "print(f\"All pseudo-labels saved to: {pseudo_label_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Backup pseudo labels\n",
        "!cp -r  /content/pseudo_labels_dice /content/drive/MyDrive/Semantic_Segmentation/pseudo_labels_backup/"
      ],
      "metadata": {
        "id": "6X14q2fdO7Ds"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "KWAJSwpa6lCr"
      },
      "outputs": [],
      "source": [
        "#!mkdir /content/pseudo_labels_dice\n",
        "!cp -r /content/drive/MyDrive/Semantic_Segmentation/pseudo_labels_backup/ /content/pseudo_labels_dice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1riuPCR670v",
        "outputId": "6ba058bf-973b-4223-ccb0-aaf84817cd75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['02492.png', '01407.png', '00345.png', '01436.png', '01773.png']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "print(os.listdir(\"/content/datasets/GTA5/labels\")[:5])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "IPD_6YAhuDA5"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import os\n",
        "import torch\n",
        "import random\n",
        "\n",
        "# Transforms\n",
        "transform_image = transforms.Compose([\n",
        "    transforms.Resize((720, 1280)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "transform_label = transforms.Compose([\n",
        "    transforms.Resize((720, 1280), interpolation=Image.NEAREST),\n",
        "    transforms.PILToTensor()\n",
        "])\n",
        "\n",
        "# GTA5 Dataset\n",
        "class GTA5Dataset(Dataset):\n",
        "    def __init__(self, root, transform_img, transform_lbl):\n",
        "        self.img_dir = os.path.join(root, \"images\")\n",
        "        self.lbl_dir = os.path.join(root, \"labels\")\n",
        "        self.imgs = sorted(os.listdir(self.img_dir))\n",
        "        self.transform_img = transform_img\n",
        "        self.transform_lbl = transform_lbl\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "    #def __getitem__(self, idx):\n",
        "     #   img_path = os.path.join(self.img_dir, self.imgs[idx])\n",
        "      #  lbl_path = os.path.join(self.lbl_dir, self.imgs[idx].replace(\"_leftImg8bit.png\", \"_gtFine_labelIds.png\"))\n",
        "\n",
        "       # img = Image.open(img_path).convert(\"RGB\")\n",
        "        #lbl = Image.open(lbl_path)\n",
        "\n",
        "       # if self.transform_img:\n",
        "        #    img = self.transform_img(img)\n",
        "        #if self.transform_lbl:\n",
        "         #   lbl = self.transform_lbl(lbl).squeeze().long()\n",
        "\n",
        "     #   return img, lbl\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "           img_name = self.imgs[idx]                          # e.g., '00878_leftImg8bit.png'\n",
        "           lbl_name = img_name.replace(\"_leftImg8bit.png\", \".png\")  # → '00878.png'\n",
        "\n",
        "           img_path = os.path.join(self.img_dir, img_name)\n",
        "           lbl_path = os.path.join(self.lbl_dir, lbl_name)\n",
        "\n",
        "           img = Image.open(img_path).convert(\"RGB\")\n",
        "           lbl = Image.open(lbl_path)\n",
        "\n",
        "           if self.transform_img:\n",
        "               img = self.transform_img(img)\n",
        "           if self.transform_lbl:\n",
        "               lbl = self.transform_lbl(lbl).squeeze().long()\n",
        "\n",
        "           return img, lbl\n",
        "\n",
        "\n",
        "# Cityscapes Val Dataset\n",
        "class CityscapesValDataset(Dataset):\n",
        "    def __init__(self, root, transform_img, transform_lbl):\n",
        "        self.img_dir = os.path.join(root, \"leftImg8bit\", \"val\")\n",
        "        self.lbl_dir = os.path.join(root, \"gtFine\", \"val\")\n",
        "        self.imgs = []\n",
        "        for city in os.listdir(self.img_dir):\n",
        "            for fname in os.listdir(os.path.join(self.img_dir, city)):\n",
        "                if fname.endswith(\"_leftImg8bit.png\"):\n",
        "                    self.imgs.append(os.path.join(city, fname))\n",
        "        self.transform_img = transform_img\n",
        "        self.transform_lbl = transform_lbl\n",
        "        self.root = root\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_rel = self.imgs[idx]\n",
        "        city, fname = img_rel.split('/')\n",
        "        lbl_name = fname.replace(\"_leftImg8bit.png\", \"_gtFine_labelTrainIds.png\")\n",
        "\n",
        "        img = Image.open(os.path.join(self.img_dir, city, fname)).convert(\"RGB\")\n",
        "        lbl = Image.open(os.path.join(self.lbl_dir, city, lbl_name))\n",
        "\n",
        "        if self.transform_img:\n",
        "            img = self.transform_img(img)\n",
        "        if self.transform_lbl:\n",
        "            lbl = self.transform_lbl(lbl).squeeze().long()\n",
        "\n",
        "        return img, lbl\n",
        "\n",
        "# DACS Dataset\n",
        "class DACSDataset(Dataset):\n",
        "    def __init__(self, gta5_dataset, cityscapes_root, pseudo_root, transform_img, transform_lbl, ignore_index=255):\n",
        "        self.gta5_dataset = gta5_dataset\n",
        "        self.transform_img = transform_img\n",
        "        self.transform_lbl = transform_lbl\n",
        "        self.ignore_index = ignore_index\n",
        "\n",
        "        self.city_imgs = []\n",
        "        self.pseudo_labels = []\n",
        "        for city in os.listdir(os.path.join(cityscapes_root, \"leftImg8bit\", \"train\")):\n",
        "            city_path = os.path.join(cityscapes_root, \"leftImg8bit\", \"train\", city)\n",
        "            for fname in os.listdir(city_path):\n",
        "                if fname.endswith(\"_leftImg8bit.png\"):\n",
        "                    self.city_imgs.append(os.path.join(city_path, fname))\n",
        "                    pseudo = os.path.join(pseudo_root, fname.replace(\"_leftImg8bit.png\", \"_pseudo_label.png\"))\n",
        "                    self.pseudo_labels.append(pseudo)\n",
        "\n",
        "    def __len__(self):\n",
        "        return min(len(self.gta5_dataset), len(self.city_imgs))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src_img, src_lbl = self.gta5_dataset[idx]\n",
        "        tgt_img = Image.open(self.city_imgs[idx]).convert(\"RGB\")\n",
        "        tgt_lbl = Image.open(self.pseudo_labels[idx])\n",
        "\n",
        "        tgt_img = self.transform_img(tgt_img)\n",
        "        tgt_lbl = self.transform_lbl(tgt_lbl).squeeze().long()\n",
        "        tgt_lbl = torch.clamp(tgt_lbl, 0, 18)\n",
        "\n",
        "        # Clamp src_lbl to valid range or ignore index\n",
        "        src_lbl = torch.clamp(src_lbl, 0, 18) # Clamp valid classes\n",
        "        src_lbl[src_lbl > 18] = self.ignore_index # Set values > 18 to ignore_index\n",
        "\n",
        "        # ClassMix\n",
        "        classes = torch.unique(src_lbl)\n",
        "        classes = classes[classes != self.ignore_index]\n",
        "        selected = classes[torch.randperm(len(classes))[:len(classes)//2]] if len(classes) > 0 else torch.tensor([], dtype=torch.long)\n",
        "        mask = torch.zeros_like(src_lbl, dtype=torch.bool)\n",
        "        for c in selected:\n",
        "            mask[src_lbl == c] = True\n",
        "        mask = mask.unsqueeze(0)\n",
        "\n",
        "        # Handle ignore index during mixing\n",
        "        mixed_lbl = torch.where(mask.squeeze(0), src_lbl, tgt_lbl)\n",
        "        # Ensure ignore index from source is preserved where mask is true\n",
        "        mixed_lbl = torch.where((mask.squeeze(0) & (src_lbl == self.ignore_index)), self.ignore_index, mixed_lbl)\n",
        "\n",
        "\n",
        "        mixed_img = tgt_img * (~mask) + src_img * mask.float()\n",
        "\n",
        "\n",
        "        return mixed_img, mixed_lbl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9i_nhngxuI_Z",
        "outputId": "c304650e-7fd0-4c8c-daa0-6f68d2c25a78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DACS ✅ torch.Size([2, 3, 720, 1280]) torch.Size([2, 720, 1280])\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Paths\n",
        "gta5_root = \"/content/datasets/GTA5\"\n",
        "cityscapes_root = \"/content/datasets/Cityscapes\"\n",
        "pseudo_label_dir = \"/content/pseudo_labels_dice\"\n",
        "\n",
        "# Initialize datasets\n",
        "gta5_dataset = GTA5Dataset(gta5_root, transform_image, transform_label)\n",
        "val_dataset  = CityscapesValDataset(cityscapes_root, transform_image, transform_label)\n",
        "dacs_dataset = DACSDataset(gta5_dataset, cityscapes_root, pseudo_label_dir, transform_image, transform_label)\n",
        "\n",
        "# DataLoaders\n",
        "gta5_loader = DataLoader(gta5_dataset, batch_size=2, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)\n",
        "val_loader  = DataLoader(val_dataset, batch_size=2, shuffle=False, num_workers=4)\n",
        "dacs_loader = DataLoader(dacs_dataset, batch_size=2, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)\n",
        "\n",
        "# Sanity check\n",
        "batch_imgs, batch_lbls = next(iter(dacs_loader))\n",
        "print(\"DACS ✅\", batch_imgs.shape, batch_lbls.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xo4hgb-LRmRK",
        "outputId": "193c9a1a-0533-4bdd-b69d-b354efc3142d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DACS ✅ torch.Size([2, 3, 720, 1280]) torch.Size([2, 720, 1280])\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Paths\n",
        "gta5_root = \"/content/datasets/GTA5\"\n",
        "cityscapes_root = \"/content/datasets/Cityscapes\"\n",
        "pseudo_label_dir = \"/content/pseudo_labels_dice\"\n",
        "\n",
        "# Initialize datasets\n",
        "gta5_dataset = GTA5Dataset(gta5_root, transform_image, transform_label)\n",
        "val_dataset  = CityscapesValDataset(cityscapes_root, transform_image, transform_label)\n",
        "dacs_dataset = DACSDataset(gta5_dataset, cityscapes_root, pseudo_label_dir, transform_image, transform_label)\n",
        "\n",
        "# Set pin_memory=False for CPU safety\n",
        "gta5_loader = DataLoader(gta5_dataset, batch_size=2, shuffle=True, num_workers=4, pin_memory=False, drop_last=True)\n",
        "val_loader  = DataLoader(val_dataset, batch_size=2, shuffle=False, num_workers=4, pin_memory=False)\n",
        "dacs_loader = DataLoader(dacs_dataset, batch_size=2, shuffle=True, num_workers=4, pin_memory=False, drop_last=True)\n",
        "\n",
        "# Sanity check\n",
        "batch_imgs, batch_lbls = next(iter(dacs_loader))\n",
        "print(\"DACS ✅\", batch_imgs.shape, batch_lbls.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoxwezC9vQkA",
        "outputId": "34118d06-b9be-4e67-9ba2-80d4ec31d1fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DACS DataLoader ✅ torch.Size([2, 3, 720, 1280]) torch.Size([2, 720, 1280])\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Dataset paths\n",
        "gta5_root = \"/content/datasets/GTA5\"\n",
        "cityscapes_root = \"/content/datasets/Cityscapes\"\n",
        "pseudo_label_dir = \"/content/pseudo_labels_dice\"  # updated pseudo-label path\n",
        "\n",
        "# Initialize datasets\n",
        "gta5_dataset = GTA5Dataset(gta5_root, transform_image, transform_label)\n",
        "val_dataset  = CityscapesValDataset(cityscapes_root, transform_image, transform_label)\n",
        "dacs_dataset = DACSDataset(gta5_dataset, cityscapes_root, pseudo_label_dir, transform_image, transform_label)\n",
        "\n",
        "# Initialize DataLoaders\n",
        "#gta5_loader = DataLoader(gta5_dataset, batch_size=2, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)\n",
        "#val_loader  = DataLoader(val_dataset, batch_size=2, shuffle=False, num_workers=4)\n",
        "#dacs_loader = DataLoader(dacs_dataset, batch_size=2, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)\n",
        "\n",
        "# Adaptive pin_memory based on device\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "gta5_loader = DataLoader(gta5_dataset, batch_size=2, shuffle=True, num_workers=4, pin_memory=False, drop_last=True)\n",
        "val_loader  = DataLoader(val_dataset, batch_size=2, shuffle=False, num_workers=4, pin_memory=False)\n",
        "dacs_loader = DataLoader(dacs_dataset, batch_size=2, shuffle=True, num_workers=4, pin_memory=False, drop_last=True)\n",
        "\n",
        "\n",
        "# Sanity check\n",
        "sample_imgs, sample_lbls = next(iter(dacs_loader))\n",
        "print(\"DACS DataLoader ✅\", sample_imgs.shape, sample_lbls.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "WiFB9C96vXIw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class HybridSegmentationLoss(nn.Module):\n",
        "    def __init__(self, weight_dice=1.0, ignore_index=255):\n",
        "        super(HybridSegmentationLoss, self).__init__()\n",
        "        self.ce_loss = nn.CrossEntropyLoss(ignore_index=ignore_index)\n",
        "        self.weight_dice = weight_dice\n",
        "        self.ignore_index = ignore_index\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        # logits: (B, C, H, W)\n",
        "        # targets: (B, H, W)\n",
        "\n",
        "        ce = self.ce_loss(logits, targets)\n",
        "\n",
        "        if self.weight_dice == 0:\n",
        "            return ce\n",
        "\n",
        "        # Convert targets to one-hot format for dice\n",
        "        num_classes = logits.shape[1]\n",
        "\n",
        "        # Create a mask for valid pixels (not ignore_index)\n",
        "        valid_mask = (targets != self.ignore_index)\n",
        "\n",
        "        # Apply mask to targets and logits for dice calculation\n",
        "        # We only consider valid pixels for dice loss\n",
        "        targets_dice = targets[valid_mask]\n",
        "        logits_dice = logits.permute(0, 2, 3, 1)[valid_mask] # (N_valid, C)\n",
        "\n",
        "        if targets_dice.numel() == 0: # Handle case where no valid pixels exist\n",
        "            return ce\n",
        "\n",
        "        # Clamp valid targets to the range [0, num_classes - 1] before one_hot\n",
        "        targets_dice_clamped = torch.clamp(targets_dice, 0, num_classes - 1)\n",
        "\n",
        "        targets_one_hot = F.one_hot(targets_dice_clamped, num_classes=num_classes).float() # (N_valid, C)\n",
        "\n",
        "        probs = torch.softmax(logits_dice, dim=-1) # (N_valid, C)\n",
        "\n",
        "        # Dice calculation on valid pixels\n",
        "        eps = 1e-7\n",
        "        intersection = (probs * targets_one_hot).sum(dim=0)\n",
        "        union = probs.sum(dim=0) + targets_one_hot.sum(dim=0)\n",
        "        dice = 1 - (2 * intersection + eps) / (union + eps)\n",
        "        dice_loss = dice.mean()\n",
        "\n",
        "        return ce + self.weight_dice * dice_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjXrf1bMxH4P",
        "outputId": "dfd627e2-116f-43dd-b9c2-e8a0bc9d635a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Device Name: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "print(f\"Using device: {device}\")\n",
        "print(\"Device Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Tt3pWO24onS",
        "outputId": "4776ca38-5b56-4a6b-c469-173ec30ec712"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dummy tensor successfully moved to GPU\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    dummy = torch.randn(1, 3, 720, 1280).to(device)\n",
        "    print(\"Dummy tensor successfully moved to GPU\")\n",
        "except RuntimeError as e:\n",
        "    print(\"Failed to move dummy tensor to GPU:\", e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "WjTWP1he5Ia5"
      },
      "outputs": [],
      "source": [
        "#device = torch.device(\"cpu\")\n",
        "#print(\"Switched to:\", device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "GJuosLjQSi_v"
      },
      "outputs": [],
      "source": [
        "gta5_loader = DataLoader(gta5_dataset, batch_size=2, shuffle=True, num_workers=4, pin_memory=False, drop_last=True)\n",
        "val_loader  = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=4, pin_memory=False)\n",
        "dacs_loader = DataLoader(dacs_dataset, batch_size=2, shuffle=True, num_workers=4, pin_memory=False, drop_last=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Qa2nMfq5TB95"
      },
      "outputs": [],
      "source": [
        "#model = BiSeNet(num_classes=19, context_path='resnet18').to(device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srWmLm8GPZuM",
        "outputId": "b8224a49-3a4d-4517-c166-a5a6e3a5e85a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files found: ['munster_000154_000019_gtFine_color.png', 'munster_000132_000019_gtFine_color.png', 'munster_000063_000019_gtFine_labelTrainIds.png', 'munster_000026_000019_gtFine_color.png', 'munster_000145_000019_gtFine_color.png']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "val_city_dir = \"/content/datasets/Cityscapes/gtFine/val/munster\"\n",
        "files = os.listdir(val_city_dir)\n",
        "print(\"Files found:\", files[:5])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NDHIGobvamB",
        "outputId": "ad1be560-0893-4af0-e0c9-67b498270aea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resumed training from epoch 48\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 49 | Train Loss: 1.0134 | Val Loss: 4.8244\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 50 | Train Loss: 1.0064 | Val Loss: 5.4947\n",
            "Training Complete\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from tqdm import tqdm\n",
        "\n",
        "# === CONFIG ===\n",
        "num_epochs = 50\n",
        "save_dir = \"/content/drive/MyDrive/Semantic_Segmentation/extension_checkpoints\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "best_model_path = os.path.join(save_dir, \"best_model_dacs_dice.pth\")\n",
        "checkpoint_path = os.path.join(save_dir, \"last_checkpoint.pth\")\n",
        "ignore_index = 255 # Define ignore index here\n",
        "\n",
        "# === MODEL ===\n",
        "model = BiSeNet(num_classes=19, context_path='resnet18').to(device)\n",
        "\n",
        "# Add CUDA synchronization and error checking (keeping for now, though the new error is different)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.synchronize()\n",
        "    try:\n",
        "        torch.cuda.current_stream().synchronize()\n",
        "    except RuntimeError as e:\n",
        "        print(f\"CUDA error after model to device: {e}\")\n",
        "\n",
        "\n",
        "        pass # Use pass to continue execution if it was just a warning\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
        "scaler = GradScaler()\n",
        "criterion = HybridSegmentationLoss(weight_dice=1.0, ignore_index=ignore_index).to(device)\n",
        "val_criterion = torch.nn.CrossEntropyLoss(ignore_index=ignore_index).to(device) # Set ignore_index for validation loss and move to device\n",
        "\n",
        "# === RESUME CHECKPOINT IF EXISTS ===\n",
        "start_epoch = 0\n",
        "best_val_loss = float(\"inf\")\n",
        "if os.path.exists(checkpoint_path):\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    scaler.load_state_dict(checkpoint['scaler'])\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "    best_val_loss = checkpoint['best_val_loss']\n",
        "    print(f\"Resumed training from epoch {start_epoch}\")\n",
        "\n",
        "# === TRAINING LOOP ===\n",
        "for epoch in range(start_epoch, num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    loop = tqdm(dacs_loader, desc=f\"Epoch {epoch+1}\", leave=False)\n",
        "\n",
        "    for imgs, labels in loop:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        with autocast():\n",
        "            logits = model(imgs)[0]\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        loop.set_postfix(train_loss=loss.item())\n",
        "\n",
        "    avg_train_loss = total_loss / len(dacs_loader)\n",
        "\n",
        "    # === VALIDATION ===\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for v_imgs, v_labels in val_loader:\n",
        "            v_imgs, v_labels = v_imgs.to(device), v_labels.to(device)\n",
        "            v_logits = model(v_imgs)[0]\n",
        "\n",
        "            # Add a check for the tensor's dimensions\n",
        "            if v_logits.dim() == 3:\n",
        "              v_logits = v_logits.unsqueeze(0) # Adds the batch dimension back in\n",
        "\n",
        "            # Reshape logits and labels for the loss function\n",
        "            v_logits = v_logits.permute(0, 2, 3, 1).reshape(-1, 19)\n",
        "            v_labels = v_labels.reshape(-1)\n",
        "\n",
        "            v_loss = val_criterion(v_logits, v_labels)\n",
        "            val_loss += v_loss.item()\n",
        "    val_loss /= len(val_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    # === SAVE BEST MODEL ===\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(f\"Saved Best Model at Epoch {epoch+1}\")\n",
        "\n",
        "    # === SAVE CHECKPOINT ===\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "        'scaler': scaler.state_dict(),\n",
        "        'best_val_loss': best_val_loss\n",
        "    }, checkpoint_path)\n",
        "\n",
        "print(\"Training Complete\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from models.bisenet.build_bisenet import BiSeNet\n",
        "\n",
        "# Device setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize BiSeNet (same setup as training)\n",
        "model = BiSeNet(19, 'resnet18').to(device)\n",
        "\n",
        "# Load best trained weights after DACS\n",
        "checkpoint_path = \"/content/drive/MyDrive/Semantic_Segmentation/extension_checkpoints/best_model_dacs_dice.pth\"\n",
        "state_dict = torch.load(checkpoint_path, map_location=device)\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "# Set to eval mode\n",
        "model.eval()\n",
        "print(\" Best DACS-trained model loaded successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82OCNWmTupBT",
        "outputId": "756dc281-55f0-478c-f4ff-589e91141841"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Best DACS-trained model loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "class CityscapesValDataset(Dataset):\n",
        "    def __init__(self, img_root, lbl_root, transform=None, target_transform=None):\n",
        "        self.img_root = img_root\n",
        "        self.lbl_root = lbl_root\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        self.img_paths = []\n",
        "        self.lbl_paths = []\n",
        "\n",
        "        for city in os.listdir(img_root):\n",
        "            img_dir = os.path.join(img_root, city)\n",
        "            lbl_dir = os.path.join(lbl_root, city)\n",
        "            for file in os.listdir(img_dir):\n",
        "                if file.endswith(\"_leftImg8bit.png\"):\n",
        "                    self.img_paths.append(os.path.join(img_dir, file))\n",
        "                    lbl_file = file.replace(\"_leftImg8bit.png\", \"_gtFine_labelTrainIds.png\")\n",
        "                    self.lbl_paths.append(os.path.join(lbl_dir, lbl_file))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.img_paths[idx]).convert(\"RGB\")\n",
        "        lbl = Image.open(self.lbl_paths[idx])\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        if self.target_transform:\n",
        "            lbl = self.target_transform(lbl)\n",
        "\n",
        "        return img, lbl\n",
        "\n",
        "# Define transforms\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((720, 1280)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "lbl_transform = transforms.Compose([\n",
        "    transforms.Resize((720, 1280), interpolation=Image.NEAREST),\n",
        "    transforms.Lambda(lambda x: torch.from_numpy(np.array(x)).long())\n",
        "])\n",
        "\n",
        "# Create dataset and dataloader\n",
        "val_dataset = CityscapesValDataset(\n",
        "    img_root=\"/content/datasets/Cityscapes/leftImg8bit/val\",\n",
        "    lbl_root=\"/content/datasets/Cityscapes/gtFine/val\",\n",
        "    transform=val_transform,\n",
        "    target_transform=lbl_transform\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
        "print(\"Validation DataLoader ready.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ou-PIuBku5Dm",
        "outputId": "fbe93c55-4d1b-4866-d9ba-60f71f7d8df8"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation DataLoader ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Initialize confusion matrix\n",
        "NUM_CLASSES = 19\n",
        "conf_matrix = np.zeros((NUM_CLASSES, NUM_CLASSES), dtype=np.int64)\n",
        "\n",
        "def fast_hist(pred, label, num_classes):\n",
        "    # Print shapes before applying mask\n",
        "    #print(\"Shape of pred in fast_hist:\", pred.shape)\n",
        "    #print(\"Shape of label in fast_hist:\", label.shape)\n",
        "    mask = (label >= 0) & (label < num_classes)\n",
        "    #print(\"Shape of mask in fast_hist:\", mask.shape)\n",
        "\n",
        "    # Apply mask before flattening\n",
        "    pred = pred[mask]\n",
        "    label = label[mask]\n",
        "\n",
        "    hist = np.bincount(\n",
        "        num_classes * label.astype(int) + pred.astype(int),\n",
        "        minlength=num_classes**2\n",
        "    ).reshape(num_classes, num_classes)\n",
        "    return hist\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for imgs, lbls in tqdm(val_loader, desc=\"Evaluating\"):\n",
        "        imgs = imgs.to(device)\n",
        "        lbls = lbls.squeeze(1).cpu().numpy()  # shape: [B, H, W]\n",
        "\n",
        "        outputs = model(imgs)[0]  # This might return [19, H, W] if batch size is 1\n",
        "        # Add batch dimension if it's missing\n",
        "        if outputs.dim() == 3:\n",
        "             outputs = outputs.unsqueeze(0) # Adds the batch dimension back in\n",
        "\n",
        "        #print(\"Shape of outputs before argmax:\", outputs.shape) # Check shape here\n",
        "        preds = outputs.argmax(dim=1).cpu().numpy()  # Now should be [B, H, W]\n",
        "\n",
        "        #print(\"Shape of preds after argmax:\", preds.shape) # Check shape here\n",
        "\n",
        "        for i in range(preds.shape[0]): # Iterate through the batch using index\n",
        "            pred = preds[i] # Get individual prediction (H, W)\n",
        "            label = lbls[i] # Get individual label (H, W)\n",
        "\n",
        "            # Print shapes before calling fast_hist\n",
        "            #print(\"Shape of pred before fast_hist:\", pred.shape)\n",
        "            #print(\"Shape of label before fast_hist:\", label.shape)\n",
        "            conf_matrix += fast_hist(pred, label, NUM_CLASSES) # Pass unflattened arrays to fast_hist\n",
        "\n",
        "# Compute mIoU\n",
        "intersection = np.diag(conf_matrix)\n",
        "union = conf_matrix.sum(1) + conf_matrix.sum(0) - np.diag(conf_matrix)\n",
        "iou = intersection / np.maximum(union, 1)\n",
        "miou = np.nanmean(iou)\n",
        "\n",
        "# Compute pixel accuracy\n",
        "pixel_acc = intersection.sum() / conf_matrix.sum()\n",
        "\n",
        "print(f\"\\n Evaluation Results:\")\n",
        "print(f\"Pixel Accuracy: {pixel_acc:.4f}\")\n",
        "print(f\"Mean IoU: {miou:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TS1hA35GvDoG",
        "outputId": "5347208c-f861-49d3-b932-09a277710fbc"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 500/500 [01:33<00:00,  5.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Evaluation Results:\n",
            "Pixel Accuracy: 0.3242\n",
            "Mean IoU: 0.0759\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5538bc3",
        "outputId": "c095a561-5fc9-4c67-8205-8e51dffac1ae"
      },
      "source": [
        "# Calculate per-class IoU\n",
        "# intersection and union were already calculated in the previous cell\n",
        "\n",
        "# Get class names (assuming Cityscapes classes) - you might need to adjust this based on your specific class mapping\n",
        "class_names = [\n",
        "    \"road\", \"sidewalk\", \"building\", \"wall\", \"fence\", \"pole\",\n",
        "    \"traffic light\", \"traffic sign\", \"vegetation\", \"terrain\", \"sky\",\n",
        "    \"person\", \"rider\", \"car\", \"truck\", \"bus\", \"train\", \"motorcycle\",\n",
        "    \"bicycle\"\n",
        "]\n",
        "\n",
        "# Ensure the number of class names matches the number of classes\n",
        "if len(class_names) != NUM_CLASSES:\n",
        "    print(f\"Warning: Number of class names ({len(class_names)}) does not match NUM_CLASSES ({NUM_CLASSES}).\")\n",
        "    # Create generic names if there's a mismatch\n",
        "    class_names = [f\"class_{i}\" for i in range(NUM_CLASSES)]\n",
        "\n",
        "\n",
        "print(\"\\nPer-Class IoU:\")\n",
        "for i in range(NUM_CLASSES):\n",
        "    # Avoid division by zero for classes not present in the ground truth or predictions\n",
        "    if union[i] > 0:\n",
        "        print(f\"{class_names[i]}: {iou[i]:.4f}\")\n",
        "    else:\n",
        "        print(f\"{class_names[i]}: N/A (no instances)\")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Per-Class IoU:\n",
            "road: 0.2073\n",
            "sidewalk: 0.1622\n",
            "building: 0.5351\n",
            "wall: 0.0020\n",
            "fence: 0.0242\n",
            "pole: 0.0000\n",
            "traffic light: 0.0000\n",
            "traffic sign: 0.0010\n",
            "vegetation: 0.0000\n",
            "terrain: 0.0006\n",
            "sky: 0.0000\n",
            "person: 0.0372\n",
            "rider: 0.0002\n",
            "car: 0.4578\n",
            "truck: 0.0118\n",
            "bus: 0.0000\n",
            "train: 0.0000\n",
            "motorcycle: 0.0000\n",
            "bicycle: 0.0033\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}