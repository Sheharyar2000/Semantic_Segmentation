{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5uJZOkMebpH",
        "outputId": "c7143877-bc5a-4f4b-ab4e-e844598bbb55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Define paths\n",
        "gta5_zip = \"/content/drive/MyDrive/Semantic_Segmentation/GTA5.zip\"\n",
        "cityscapes_zip = \"/content/drive/MyDrive/Semantic_Segmentation/Cityscapes.zip\"\n",
        "\n",
        "os.makedirs(\"/content/datasets/GTA5\", exist_ok=True)\n",
        "os.makedirs(\"/content/datasets/Cityscapes\", exist_ok=True)\n",
        "\n",
        "# Unzip GTA5\n",
        "with zipfile.ZipFile(gta5_zip, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"/content/datasets/GTA5\")\n",
        "\n",
        "# Unzip Cityscapes\n",
        "with zipfile.ZipFile(cityscapes_zip, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"/content/datasets/Cityscapes\")\n",
        "\n",
        "print(\"‚úÖ Both datasets extracted.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnpbQb74e3J0",
        "outputId": "f4352c8c-8897-4384-9e72-2563a98dadf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Both datasets extracted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Fix GTA5\n",
        "if os.path.exists(\"/content/datasets/GTA5/GTA5\"):\n",
        "    shutil.move(\"/content/datasets/GTA5/GTA5/images\", \"/content/datasets/GTA5/images\")\n",
        "    shutil.move(\"/content/datasets/GTA5/GTA5/labels\", \"/content/datasets/GTA5/labels\")\n",
        "    shutil.rmtree(\"/content/datasets/GTA5/GTA5\")\n",
        "    print(\"‚úÖ Fixed GTA5 folder structure\")\n",
        "\n",
        "# Fix Cityscapes\n",
        "nested_city = \"/content/datasets/Cityscapes/Cityscapes/Cityspaces\"\n",
        "if os.path.exists(nested_city):\n",
        "    shutil.move(os.path.join(nested_city, \"images\"), \"/content/datasets/Cityscapes/leftImg8bit\")\n",
        "    shutil.move(os.path.join(nested_city, \"gtFine\"), \"/content/datasets/Cityscapes/gtFine\")\n",
        "    shutil.rmtree(\"/content/datasets/Cityscapes/Cityscapes\")\n",
        "    print(\"‚úÖ Fixed Cityscapes folder structure\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnA8M5tFfiAF",
        "outputId": "8a6031a5-c33b-45de-ef49-e255ec8aa139"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Fixed GTA5 folder structure\n",
            "‚úÖ Fixed Cityscapes folder structure\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup ‚Äî Imports & Transforms"
      ],
      "metadata": {
        "id": "rCaD0L07iEC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "UkralCgUiEjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GTA5 and Cityscapes root paths\n",
        "gta5_root = \"/content/datasets/GTA5\"\n",
        "cityscapes_root = \"/content/datasets/Cityscapes\"\n",
        "\n",
        "# Confirm directories exist\n",
        "print(\"GTA5 Image Path Exists?\", os.path.exists(os.path.join(gta5_root, \"images\")))\n",
        "print(\"GTA5 Label Path Exists?\", os.path.exists(os.path.join(gta5_root, \"labels\")))\n",
        "print(\"Cityscapes leftImg8bit Exists?\", os.path.exists(os.path.join(cityscapes_root, \"leftImg8bit\")))\n",
        "print(\"Cityscapes gtFine Exists?\", os.path.exists(os.path.join(cityscapes_root, \"gtFine\")))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTLvW13KiU4R",
        "outputId": "6693a1ee-225f-41ee-f72f-782860a9ce98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GTA5 Image Path Exists? True\n",
            "GTA5 Label Path Exists? True\n",
            "Cityscapes leftImg8bit Exists? True\n",
            "Cityscapes gtFine Exists? True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define GTA5Dataset Class"
      ],
      "metadata": {
        "id": "93ZKu0sFiwCB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "class GTA5Dataset(Dataset):\n",
        "    def __init__(self, root, transform=None, target_transform=None):\n",
        "        self.root = root\n",
        "        self.image_dir = os.path.join(root, \"images\")\n",
        "        self.label_dir = os.path.join(root, \"labels\")\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "        self.images = sorted(os.listdir(self.image_dir))\n",
        "        self.labels = sorted(os.listdir(self.label_dir))\n",
        "        assert len(self.images) == len(self.labels), \"Mismatch between images and labels\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.image_dir, self.images[idx])\n",
        "        label_path = os.path.join(self.label_dir, self.labels[idx])\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        label = Image.open(label_path)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "\n",
        "        return image, label\n"
      ],
      "metadata": {
        "id": "nrR_Riz2iwjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define CityscapesDataset Class (for validation)"
      ],
      "metadata": {
        "id": "HZ_fawk7i_nd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "class CityscapesDataset(Dataset):\n",
        "    def __init__(self, root, split='val', transform=None, target_transform=None):\n",
        "        self.root = root\n",
        "        self.split = split\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "        self.img_dir = os.path.join(root, \"leftImg8bit\", split)\n",
        "        self.label_dir = os.path.join(root, \"gtFine\", split)\n",
        "\n",
        "        self.img_paths = []\n",
        "        self.label_paths = []\n",
        "\n",
        "        for city in os.listdir(self.img_dir):\n",
        "            img_city_path = os.path.join(self.img_dir, city)\n",
        "            label_city_path = os.path.join(self.label_dir, city)\n",
        "\n",
        "            for file_name in os.listdir(img_city_path):\n",
        "                if file_name.endswith(\"_leftImg8bit.png\"):\n",
        "                    img_path = os.path.join(img_city_path, file_name)\n",
        "                    label_name = file_name.replace(\"_leftImg8bit.png\", \"_gtFine_labelTrainIds.png\")\n",
        "                    label_path = os.path.join(label_city_path, label_name)\n",
        "\n",
        "                    self.img_paths.append(img_path)\n",
        "                    self.label_paths.append(label_path)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.img_paths[idx]).convert(\"RGB\")\n",
        "        label = Image.open(self.label_paths[idx])\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "\n",
        "        return image, label\n"
      ],
      "metadata": {
        "id": "E42umzK5i_99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DACSDataset Class Implementation"
      ],
      "metadata": {
        "id": "PtPk_iYdkcSY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import os\n",
        "import torch\n",
        "\n",
        "class DACSDataset(Dataset):\n",
        "    def __init__(self, gta5_dataset, cityscapes_root, transform_img, transform_label):\n",
        "        self.gta5_dataset   = gta5_dataset\n",
        "        self.cityscapes_root = cityscapes_root\n",
        "        self.transform_img  = transform_img\n",
        "        self.transform_label = transform_label\n",
        "        self.pseudo_dir     = \"/content/pseudo_labels\"\n",
        "\n",
        "        # Paths to Cityscapes train images\n",
        "        self.city_imgs = sorted([\n",
        "            os.path.join(cityscapes_root, \"leftImg8bit\", \"train\", city, f)\n",
        "            for city in os.listdir(os.path.join(cityscapes_root, \"leftImg8bit\", \"train\"))\n",
        "            for f in os.listdir(os.path.join(cityscapes_root, \"leftImg8bit\", \"train\", city))\n",
        "            if f.endswith(\"_leftImg8bit.png\")\n",
        "        ])\n",
        "\n",
        "        # Matching pseudo-label paths\n",
        "        self.pseudo_labels = sorted([\n",
        "            os.path.join(self.pseudo_dir, os.path.basename(p).replace(\"_leftImg8bit.png\",\n",
        "                                                                      \"_pseudo_label.png\"))\n",
        "            for p in self.city_imgs\n",
        "        ])\n",
        "\n",
        "        assert len(self.city_imgs) == len(self.pseudo_labels), \"Image / pseudo-label count mismatch\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return min(len(self.gta5_dataset), len(self.city_imgs))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # --- Source (already transformed in GTA5Dataset) ---\n",
        "        src_img, src_label = self.gta5_dataset[idx]          # tensors already\n",
        "\n",
        "        # --- Target (needs transforms) ---\n",
        "        tgt_img   = Image.open(self.city_imgs[idx]).convert(\"RGB\")\n",
        "        tgt_label = Image.open(self.pseudo_labels[idx])      # PIL Image\n",
        "\n",
        "        if self.transform_img:\n",
        "            tgt_img = self.transform_img(tgt_img)\n",
        "\n",
        "        if isinstance(tgt_label, Image.Image) and self.transform_label:\n",
        "            tgt_label = self.transform_label(tgt_label).squeeze().long()\n",
        "\n",
        "        # --- Clamp labels to valid range [0, 18] ---\n",
        "        src_label = torch.clamp(src_label.squeeze().long(), 0, 18)\n",
        "        tgt_label = torch.clamp(tgt_label, 0, 18)\n",
        "\n",
        "        # --- ClassMix mask (random half of source classes) ---\n",
        "        classes  = torch.unique(src_label[src_label != 255])\n",
        "        if len(classes):\n",
        "            chosen = classes[torch.randint(0, len(classes), (len(classes)//2 + 1,))]\n",
        "        else:\n",
        "            chosen = torch.tensor([], dtype=torch.long)\n",
        "        mask = torch.zeros_like(src_label, dtype=torch.bool)\n",
        "        for c in chosen:\n",
        "            mask |= (src_label == c)\n",
        "        mask = mask.unsqueeze(0)          # shape (1,H,W) for broadcasting\n",
        "\n",
        "        # --- Mix images and labels ---\n",
        "        mixed_img   = torch.where(mask, src_img, tgt_img)\n",
        "        mixed_label = torch.where(mask.squeeze(0), src_label, tgt_label)\n",
        "\n",
        "        return mixed_img, mixed_label\n"
      ],
      "metadata": {
        "id": "Ct-1GCQmLqfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image and Label Transformations"
      ],
      "metadata": {
        "id": "dgkUcsO_mJMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "# ‚úÖ Mean and std for normalization (ImageNet stats)\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std  = [0.229, 0.224, 0.225]\n",
        "\n",
        "# ‚úÖ Image Transform\n",
        "transform_img = transforms.Compose([\n",
        "    transforms.Resize((720, 1280)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=mean, std=std),\n",
        "])\n",
        "\n",
        "# ‚úÖ Label Transform (Just resize + convert to tensor, no normalization)\n",
        "transform_label = transforms.Compose([\n",
        "    transforms.Resize((720, 1280), interpolation=transforms.InterpolationMode.NEAREST),\n",
        "    transforms.PILToTensor(),  # Keeps integer label values intact\n",
        "])\n"
      ],
      "metadata": {
        "id": "ZKL2759UmJlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sanity Check"
      ],
      "metadata": {
        "id": "8_u_P0Tamd94"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "frankfurt_dir = \"/content/datasets/Cityscapes/gtFine/val/frankfurt\"\n",
        "files = os.listdir(frankfurt_dir)\n",
        "\n",
        "print(f\"üìÅ Total files in frankfurt/: {len(files)}\")\n",
        "print(\"üìù Sample files:\")\n",
        "print(files[:10])  # Print first 10 file names\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBxUAKU-nmHr",
        "outputId": "ddf198b4-1256-47c0-95e7-a538951b09f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÅ Total files in frankfurt/: 534\n",
            "üìù Sample files:\n",
            "['frankfurt_000000_012868_gtFine_labelTrainIds.png', 'frankfurt_000001_020693_gtFine_labelTrainIds.png', 'frankfurt_000000_012009_gtFine_labelTrainIds.png', 'frankfurt_000001_032018_gtFine_labelTrainIds.png', 'frankfurt_000001_051737_gtFine_labelTrainIds.png', 'frankfurt_000001_042384_gtFine_labelTrainIds.png', 'frankfurt_000001_029236_gtFine_color.png', 'frankfurt_000001_033655_gtFine_labelTrainIds.png', 'frankfurt_000001_038645_gtFine_labelTrainIds.png', 'frankfurt_000001_075984_gtFine_labelTrainIds.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate datasets (use a small subset for quick testing)\n",
        "gta5_dataset = GTA5Dataset(\"/content/datasets/GTA5\", transform=transform_img, target_transform=transform_label)\n",
        "cityscapes_dataset = CityscapesDataset(\"/content/datasets/Cityscapes\", split=\"val\", transform=transform_img, target_transform=transform_label)\n",
        "\n",
        "# Fetch one sample from each\n",
        "gta5_img, gta5_label = gta5_dataset[0]\n",
        "city_img, city_label = cityscapes_dataset[0]\n",
        "\n",
        "# Print stats\n",
        "print(\"GTA5 Sample:\")\n",
        "print(\"  Image shape:\", gta5_img.shape)\n",
        "print(\"  Label shape:\", gta5_label.shape)\n",
        "print(\"  Label min/max:\", gta5_label.min().item(), gta5_label.max().item())\n",
        "print(\"  Unique classes:\", torch.unique(gta5_label))\n",
        "\n",
        "print(\"\\n Cityscapes Val Sample:\")\n",
        "print(\"  Image shape:\", city_img.shape)\n",
        "print(\"  Label shape:\", city_label.shape)\n",
        "print(\"  Label min/max:\", city_label.min().item(), city_label.max().item())\n",
        "print(\"  Unique classes:\", torch.unique(city_label))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AM43G4aZmgF6",
        "outputId": "9d0de88c-4662-473f-dc31-e512916b8096"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GTA5 Sample:\n",
            "  Image shape: torch.Size([3, 720, 1280])\n",
            "  Label shape: torch.Size([1, 720, 1280])\n",
            "  Label min/max: 0 27\n",
            "  Unique classes: tensor([ 0,  5,  6,  7, 11, 12, 13, 15, 17, 21, 22, 23, 24, 27],\n",
            "       dtype=torch.uint8)\n",
            "\n",
            " Cityscapes Val Sample:\n",
            "  Image shape: torch.Size([3, 720, 1280])\n",
            "  Label shape: torch.Size([1, 720, 1280])\n",
            "  Label min/max: 0 255\n",
            "  Unique classes: tensor([  0,   1,   2,   5,   6,   7,   8,   9,  10,  11,  13, 255],\n",
            "       dtype=torch.uint8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize GTA5 Dataloader\n"
      ],
      "metadata": {
        "id": "GgXUZnG9pqzJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Step 3.1: Initialize GTA5 Dataloader\n",
        "batch_size = 2\n",
        "num_workers = 2\n",
        "pin_memory = True\n",
        "drop_last = True\n",
        "shuffle = True\n",
        "\n",
        "gta5_loader = DataLoader(\n",
        "    gta5_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=shuffle,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=pin_memory,\n",
        "    drop_last=drop_last\n",
        ")\n",
        "\n",
        "# Sanity check\n",
        "gta5_imgs, gta5_labels = next(iter(gta5_loader))\n",
        "print(\"GTA5 Dataloader Sanity Check:\")\n",
        "print(\"  Image batch shape:\", gta5_imgs.shape)\n",
        "print(\"  Label batch shape:\", gta5_labels.shape)\n",
        "print(\"  Unique labels in batch:\", torch.unique(gta5_labels))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRpmmDAsptJr",
        "outputId": "0041ddd8-fba4-4ba8-b8c6-2822c34c7856"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GTA5 Dataloader Sanity Check:\n",
            "  Image batch shape: torch.Size([2, 3, 720, 1280])\n",
            "  Label batch shape: torch.Size([2, 1, 720, 1280])\n",
            "  Unique labels in batch: tensor([ 0,  1,  4,  5,  6,  7,  8, 11, 12, 13, 15, 17, 19, 20, 21, 22, 23, 24,\n",
            "        26, 27], dtype=torch.uint8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize the Cityscapes Validation Dataloader (val_loader)"
      ],
      "metadata": {
        "id": "SY7mVzZpqU0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "import torch\n",
        "\n",
        "# For Cityscapes validation images\n",
        "image_transform = transforms.Compose([\n",
        "    transforms.Resize((720, 1280)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# For validation masks\n",
        "target_transform = transforms.Compose([\n",
        "    transforms.Resize((720, 1280), interpolation=TF.InterpolationMode.NEAREST),\n",
        "    transforms.PILToTensor()\n",
        "])\n"
      ],
      "metadata": {
        "id": "FzvNL6W9qk2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Validation dataset (Cityscapes val)\n",
        "cityscapes_val_dataset = CityscapesDataset(\n",
        "    root=\"/content/datasets/Cityscapes\",\n",
        "    split='val',\n",
        "    transform=image_transform,\n",
        "    target_transform=target_transform\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    cityscapes_val_dataset,\n",
        "    batch_size=2,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "# üîç Sanity check\n",
        "val_imgs, val_masks = next(iter(val_loader))\n",
        "\n",
        "print(\"üîé Cityscapes Val Sample:\")\n",
        "print(f\"Image batch shape: {val_imgs.shape}\")\n",
        "print(f\"Label batch shape: {val_masks.shape}\")\n",
        "print(f\"Unique labels: {torch.unique(val_masks[0])}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5q71X_1qVXt",
        "outputId": "f35b60c3-fd3f-48ff-8fb3-72ba409e0cde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîé Cityscapes Val Sample:\n",
            "Image batch shape: torch.Size([2, 3, 720, 1280])\n",
            "Label batch shape: torch.Size([2, 1, 720, 1280])\n",
            "Unique labels: tensor([  0,   1,   2,   5,   6,   7,   8,   9,  10,  11,  13, 255],\n",
            "       dtype=torch.uint8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DACS DataLoader"
      ],
      "metadata": {
        "id": "NUX41RNXq3bR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Initialize the DACS dataset\n",
        "dacs_dataset = DACSDataset(\n",
        "    gta5_dataset=gta5_dataset,\n",
        "    cityscapes_root=\"/content/datasets/Cityscapes\",\n",
        "    transform_img=image_transform,\n",
        "    transform_label=target_transform\n",
        ")\n",
        "\n",
        "# DACS loader (source + pseudo target mixed)\n",
        "dacs_loader = DataLoader(\n",
        "    dacs_dataset,\n",
        "    batch_size=2,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1IYUEsgVq3uJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clone and Set Up BiSeNet Repository"
      ],
      "metadata": {
        "id": "i4XpTxCIsLay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the BiSeNet repo (custom or official)\n",
        "!git clone https://github.com/Gabrysse/MLDL2024_project1.git\n",
        "\n",
        "\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"/content/MLDL2024_project1\")\n",
        "\n",
        "# ‚úÖ Test import\n",
        "from models.bisenet.build_bisenet import BiSeNet\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(\"BiSeNet repo cloned and model imported successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1n1xAKdRscqz",
        "outputId": "9458bffe-fadc-481b-a4a5-a1109dc57fc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'MLDL2024_project1' already exists and is not an empty directory.\n",
            "BiSeNet repo cloned and model imported successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from models.bisenet.build_bisenet import BiSeNet\n",
        "import torch\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Build model (uses ResNet-18 by default as per repo structure)\n",
        "model = BiSeNet(19, 'resnet18')\n",
        "model = model.to(device)\n",
        "\n",
        "# Load FDA-trained weights\n",
        "checkpoint_path = \"/content/drive/MyDrive/Semantic_Segmentation/bisenet_gta5_fda_final.pth\"\n",
        "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "model.load_state_dict(checkpoint)\n",
        "\n",
        "# Set to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "print(\"‚úÖ BiSeNet loaded with FDA-trained weights.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nGWHgmwv97S",
        "outputId": "4a3056e8-5c3d-48fa-8726-ffd024d596b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 44.7M/44.7M [00:00<00:00, 134MB/s]\n",
            "Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 171M/171M [00:01<00:00, 106MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ BiSeNet loaded with FDA-trained weights.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply FDA-trained BiSeNet model to Generate Pseudo-Labels"
      ],
      "metadata": {
        "id": "vu49OGMi1DSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "# ‚úÖ Path to Cityscapes train images\n",
        "cityscapes_root = \"/content/datasets/Cityscapes\"\n",
        "image_dir = os.path.join(cityscapes_root, \"leftImg8bit\", \"train\")\n",
        "\n",
        "# ‚úÖ Output directory for pseudo-labels\n",
        "pseudo_label_dir = \"/content/pseudo_labels\"\n",
        "os.makedirs(pseudo_label_dir, exist_ok=True)\n",
        "\n",
        "# ‚úÖ Reuse image transform\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std  = [0.229, 0.224, 0.225]\n",
        "transform_img = transforms.Compose([\n",
        "    transforms.Resize((720, 1280)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=mean, std=std),\n",
        "])\n",
        "\n",
        "# ‚úÖ Iterate through all train images\n",
        "cities = os.listdir(image_dir)\n",
        "all_image_paths = []\n",
        "for city in cities:\n",
        "    city_path = os.path.join(image_dir, city)\n",
        "    for fname in os.listdir(city_path):\n",
        "        if fname.endswith(\"_leftImg8bit.png\"):\n",
        "            all_image_paths.append(os.path.join(city_path, fname))\n",
        "\n",
        "print(f\"üîç Found {len(all_image_paths)} images in Cityscapes/train\")\n",
        "\n",
        "# ‚úÖ Generate pseudo-labels\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for img_path in tqdm(all_image_paths, desc=\"Generating Pseudo-labels\"):\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        input_tensor = transform_img(img).unsqueeze(0).to(device)  # (1, 3, H, W)\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(input_tensor)[0]  # shape (1, 19, H, W)\n",
        "        pred = torch.argmax(output.squeeze(), dim=0).cpu().numpy().astype(np.uint8)\n",
        "\n",
        "        # Save pseudo-label mask\n",
        "        out_name = os.path.basename(img_path).replace(\"_leftImg8bit.png\", \"_pseudo_label.png\")\n",
        "        out_path = os.path.join(pseudo_label_dir, out_name)\n",
        "        Image.fromarray(pred).save(out_path)\n",
        "\n",
        "print(\"‚úÖ All pseudo-labels generated and saved to:\", pseudo_label_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfy2onAm1HqD",
        "outputId": "925dd920-80c5-4014-f907-7b38bdc3360e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Found 1572 images in Cityscapes/train\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Pseudo-labels: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1572/1572 [1:27:45<00:00,  3.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ All pseudo-labels generated and saved to: /content/pseudo_labels\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Re-initialize the DACS dataset"
      ],
      "metadata": {
        "id": "5WBPCHKwKOaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-initialize DACS Dataset with existing pseudo-labels\n",
        "dacs_dataset = DACSDataset(\n",
        "    gta5_dataset=gta5_dataset,\n",
        "    cityscapes_root=\"/content/datasets/Cityscapes\",\n",
        "    transform_img=transform_img,\n",
        "    transform_label=transform_label\n",
        ")\n"
      ],
      "metadata": {
        "id": "mKSuPUOsKO2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sanity Check (small batch to avoid OOM)"
      ],
      "metadata": {
        "id": "YPlwiIQQKWA7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Subset\n",
        "\n",
        "# Use a subset for quick test\n",
        "small_dacs = Subset(dacs_dataset, range(10))\n",
        "dacs_loader = DataLoader(small_dacs, batch_size=2, shuffle=False)\n",
        "\n",
        "# Grab 1 mini-batch\n",
        "mixed_imgs, mixed_labels = next(iter(dacs_loader))\n",
        "print(\"Mixed image shape:\", mixed_imgs.shape)\n",
        "print(\"Mixed label shape:\", mixed_labels.shape)\n",
        "print(\"Unique labels in sample:\", torch.unique(mixed_labels))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yM-1h_9OKWl0",
        "outputId": "6a568893-3193-40e4-d13a-7f9fb31e9e8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Mixed image shape: torch.Size([2, 3, 720, 1280])\n",
            "‚úÖ Mixed label shape: torch.Size([2, 720, 1280])\n",
            "‚úÖ Unique labels in sample: tensor([ 0,  1,  2,  3,  4,  6,  9, 10, 11, 12, 13, 14, 15, 17, 18])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = CityscapesDataset(\n",
        "    root=\"/content/datasets/Cityscapes\",\n",
        "    split='val',\n",
        "    transform=image_transform,\n",
        "    target_transform=target_transform\n",
        ")\n"
      ],
      "metadata": {
        "id": "eHYaemB0WBKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "from models.bisenet.build_bisenet import BiSeNet\n",
        "\n",
        "# === Load datasets and dataloaders (assume already defined) ===\n",
        "gta5_loader = DataLoader(gta5_dataset, batch_size=2, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, num_workers=4)\n",
        "dacs_loader = DataLoader(dacs_dataset, batch_size=2, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)\n",
        "\n",
        "# === Model, optimizer, scaler, criterion ===\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BiSeNet(19, 'resnet18').to(device)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=2.5e-4, momentum=0.9, weight_decay=1e-4)\n",
        "scaler = GradScaler()\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
        "\n",
        "# === Checkpoint logic ===\n",
        "checkpoint_path = \"/content/checkpoints/dacs_bisenet.pth\"\n",
        "os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n",
        "start_epoch = 0\n",
        "best_val_loss = float(\"inf\")\n",
        "\n",
        "if os.path.exists(checkpoint_path):\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint[\"model\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "    scaler.load_state_dict(checkpoint[\"scaler\"])\n",
        "    start_epoch = checkpoint[\"epoch\"] + 1\n",
        "    best_val_loss = checkpoint[\"best_val\"]\n",
        "    print(f\"Resumed training from epoch {start_epoch}\")\n",
        "\n",
        "# === Training loop ===\n",
        "num_epochs = 30\n",
        "for epoch in range(start_epoch, num_epochs):\n",
        "    model.train()\n",
        "    train_loop = tqdm(dacs_loader, desc=f\"Train E{epoch+1}\")\n",
        "    for imgs, labels in train_loop:\n",
        "        imgs = imgs.to(device)\n",
        "        labels = labels.to(device).long()\n",
        "        labels = torch.where(labels > 18, torch.full_like(labels, 255), labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        with autocast():\n",
        "            logits = model(imgs)[0]  # Output: [B, C, H, W]\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        train_loop.set_postfix(loss=loss.item())\n",
        "\n",
        "    # === Validation ===\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for v_imgs, v_lbls in tqdm(val_loader, desc=f\"üîç Val E{epoch+1}\"):\n",
        "            v_imgs = v_imgs.to(device)\n",
        "            v_lbls = v_lbls.to(device).long()\n",
        "            v_lbls = v_lbls.squeeze(1) if v_lbls.dim() == 4 else v_lbls\n",
        "            v_lbls = torch.where(v_lbls > 18, torch.full_like(v_lbls, 255), v_lbls)\n",
        "\n",
        "            v_out = model(v_imgs)[0]\n",
        "            if v_out.shape[0] != v_lbls.shape[0]:\n",
        "                min_b = min(v_out.shape[0], v_lbls.shape[0])\n",
        "                v_out = v_out[:min_b]\n",
        "                v_lbls = v_lbls[:min_b]\n",
        "\n",
        "            vloss_b = criterion(v_out, v_lbls)\n",
        "            val_loss += vloss_b.item()\n",
        "    val_loss /= len(val_loader)\n",
        "    print(f\"Epoch {epoch+1} Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    # === Save best model ===\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), \"/content/checkpoints/dacs_bisenet_best.pth\")\n",
        "        print(\"New best model saved.\")\n",
        "\n",
        "    # === Save checkpoint ===\n",
        "    torch.save({\n",
        "        \"epoch\": epoch,\n",
        "        \"model\": model.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict(),\n",
        "        \"scaler\": scaler.state_dict(),\n",
        "        \"best_val\": best_val_loss,\n",
        "    }, checkpoint_path)\n",
        "    print(f\"Checkpoint saved for epoch {epoch+1}\")\n"
      ],
      "metadata": {
        "id": "NPESm_ciHzAl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}